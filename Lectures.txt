Lecture1
uh, eight parts of this module, which will be on human computer interaction.
5:54
So you don't need to read any book. Well, it's always good to read books.
6:00
You don't need to read a book. Everything is in the slide materials. But there is a book which is free.
6:04
It's open access, so you can just read it. It's a free PDF. And I will make references at the end of every lecture.
6:09
If you want to find out more information on where to find that information in this book.
6:15
Okeydoke. Let's start. So basically what we're dealing with here is that the computer is seen as a tool.
6:20
And as we would see that this has deep implications. So tools are useful because tools act as amplifiers.
6:29
So any sort of tool that exists, the chatty, petty or simple things like a spreadsheet, it amplifies your ability to do things.
6:36
And it's very important to understand this amplification. So a computer is a tool.
6:44
So basically computer programs, as you know, decompose into way simpler operations.
6:51
But tools are useless if you can't control them. A tool we kind of control is fully autonomous.
6:57
So what you need is a user interface, an interface to allow you to actually control the tool.
7:02
And this is true for really nitty-gritty tools like a hammer.
7:07
And it's most certainly true for more complicated systems.
7:11
So to do this we need to sort of tackle how to actually think about these interfaces and how to use these tools.
7:15
And there are a set of challenges. So one challenge is that humans are very complex systems with lots of different subsystems.
7:22
So having language capabilities and basically the capability of conscious thought and fine motor movements,
7:30
ways of adapting to different situations is quite powerful.
7:37
But you have to understand all of these innate limitations and, uh,
7:41
and capabilities of humans in order to and to build tools that can be controllable.
7:47
And even though we know a lot, for instance, we know a lot about central aspects of human motor control,
7:52
there are still a lot of information we do not know and information we do not know.
7:57
Uh, well, it makes us not build the best tools for humans to begin with.
8:02
The second thing that is challenging is even if we do have a very good sense about how humans operate.
8:06
Uh, there are huge individual differences.
8:11
So if you pick up a human factors, uh, uh, human Factors Engineering handbook, it will have huge tables about, for instance, people's hand sizes.
8:14
So an adult human's hand side can be half my hand size or twice my hand size.
8:24
Right. The range is enormous. And that's just the hand size right. So they're huge huge amounts of individual differences.
8:30
Some of these are cultural or contextual.
8:38
And some of these is because of different experiences or different preferences or different values that we have sort of learned to deal with.
8:40
Another problem is that computers are complex, as you know from one digital circuits.
8:50
There's a lot of stuff going on there. Some computers nowadays rely on multiple course and so on.
8:55
We want to reduce the number of compute. We want to have low latency.
9:01
There's a lot of stuff that we have to think about in order to provide a good experience.
9:05
And this basically gives rise to technical constraints and how to build interfaces.
9:09
And as we will see, design a system with a high level of complexity is inherently challenging for a number of reasons we will go into later.
9:15
And the reason is that we have to basically understand systems.
9:23
So we live in a world where there are lots of subsystems that basically, uh, complete each other to build total systems.
9:28
So we do what's called a systems approach. So we need to take what people call the socio technical context into, uh, account.
9:36
Basically all the technical systems we are working with and how people are going about through organisations to do their daily work.
9:44
The strategies people have for work and so on. So even if we focus on one subsystem, right, so we build one subsystem really, really right,
9:51
which can be a very, very trivial little subsystem like a form filling application.
10:00
That subsystem will affect how people do their work and also how that subsystem operates within the wider system of the organisation itself.
10:05
So we need to understand all of these systems. So one of the most common modes of failure is to build systems and deploy them.
10:14
And then people cannot appropriate these systems okay.
10:22
So appropriation which we'll talk about later is this idea.
10:25
But in order for people to adopt systems and use them, they have to figure out how to use this system to solve their goals.
10:29
Right. So the temptation is almost to go in. Think you understand the problem?
10:36
Build a little subsystem, inject it into an organisation, be it a factory or an office.
10:41
And then there are lots of unintended consequences and people do not adopt the system.
10:46
Another challenge is about design is hard. So basically, what you kind of want to do at the high level is to create things that are effective,
10:52
efficient and safe and pleasant and so on and so forth. So a design is basically, from a theoretical point of view, quite interesting.
10:59
So you have a bunch of concerns. You want to make things beautiful, easy to use, safe efficient whatever.
11:07
Right. So these are design dimensions. And if you start adding them up you actually end up with a lot of dimensions.
11:12
Right. So you have a very high dimensional space where you can basically create instances of a design.
11:18
So you have your operating point, your specific design,
11:24
and you can move your operating point around in this multidimensional design space and basically implicitly generate hypothetical designs.
11:27
Okay. But the problem is you can't put these operating points at the optimal location.
11:34
Why not? Why can't you do an optimal design?
11:41
Perfect design. Yeah.
11:46
Uh, as I said, I think in some aspects we'll have trade-offs.
11:51
Yeah, exactly. There are trade offs, right? If I want to make a wearable device, I may want to make it as small as possible.
11:55
Right. But I may also want to maximise battery life. Right? I can't do both, but it's a trade off, right?
12:01
So while there is an optimal set of trade off decisions, there is no optimal operating point.
12:07
So this means we need to understand what all the trade offs. Right. And to do that we need to explore.
12:11
So one of the big things in design is that there hard to know what you do not know.
12:17
Okay. Another problem is what I call the engineers disease.
12:21
Basically I'm greedy. And so whatever comes first to my mind must be the best solution.
12:24
So let's go with that. Right. That's called design fixation.
12:29
And if you do that you tend to not explore the space fully and you end up with suboptimal designs.
12:32
The problem with design fixation is that it's inherently human to engage in design fixation.
12:38
So lots of studies on professional designers that show and these have been trained to avoid design fixation,
12:43
but to explore different solutions and so on and so forth. They still exhibit a huge amount of design fixation.
12:49
It's very human to basically jump to a solution.
12:55
The problem is it doesn't lead to good design.
12:58
And then human computer interaction is a little bit difficult as well because it requires so many different skills.
13:03
You need to understand something about software and increasingly software architectures as well.
13:09
You also have to have some analytical skills, for instance form a modelling and statistical analysis.
13:13
You also need some sort of design practice to avoid design fixation and be able to explore different kind of solutions.
13:19
And then you need some research skills like how do you actually talk to people.
13:25
And when you talk to people, do you trust them? What do you make of the information you're provided and how do you do a field study?
13:29
And so on and so forth. So it requires a plurality of skills, and people rarely possess the plurality of all these skills.
13:35
So the type of research people do is basically either empirical conceptual or constructive.
13:43
So empirical basically means you are trying to explain phenomena.
13:48
So this is basically science right. You want to explain empirically why things are the way we all.
13:52
For instance, we may want to discover how people learn and use features in a user interface in a particular setting.
13:57
We do some studies to understand that when you have conceptual problems, right?
14:03
So this is basically when we try to come up with hypotheses. Right.
14:08
So we come up with hypotheses or constructs.
14:11
But we can basically show that if you do things in a certain way, this kind of behaviour will be induced by that particular design.
14:14
And the third thing is constructive problems.
14:22
This basically means building systems for some purpose and then understanding how to build things in the best possible way.
14:25
It's essentially a form of engineering science. In practice, there are lots of professionals who work in these sort of roles.
14:33
Some of them actually have an image. Some of them have a psychology background.
14:42
A lot of them come from computer science.
14:46
And are what they do basically, uh, changes depending on the road and also depending on what the market actually demands.
14:48
And studying how people use and practice has itself in itself been a focus of HCI research.
14:58
Here are some common things people do. People do empirical research.
15:04
So they basically, uh,
15:08
try to understand the activities within organisations and so on and so forth for testing different systems and see how well they work.
15:10
Uh, another thing is. But a lot of people do. So basically, uh, uh, help contribute to constructive activities.
15:17
So they basically design you interfaces based on knowhow and engineer, basically build them and then eventually test them.
15:24
And another thing, big thing is to basically do evaluation.
15:35
So verifying that requirements have been met and validating that the systems are fit for purpose.
15:38
Uh, and in general, coming up with ideally new uh, system design principles based on what works and what doesn't work when these systems are deployed.
15:43
Then, as you may have found out already, that but you will find out if you start a corporate job with that.
15:52
One of the most important skills to go anywhere in a corporate job is communication, right?
15:57
You need to come in, communicate, and persuade people, right?
16:02
And this is also something that a lot of people are doing because you need to influence decision makers.
16:05
Because if you can't influence decision makers with a budget, nothing will happen in the corporate world.
16:10
Trust me. So these are some activities what people do.
16:14
Okay. So what are some fundamental concepts. So basically the way I would structure this is basically how we structure his book.
16:19
So our extension to post the first part is just the introduction to the steps that we're doing now.
16:27
And what we have here is part two Understanding People. So this basically means as humans have a set of innate capabilities cognition,
16:32
motor control and memory and so on, and tool use and practice and so on.
16:39
And it's very important to understand what are the humans capabilities, human capabilities and limitations.
16:45
The problem is knowing human capabilities and limitations is insufficient to build working systems,
16:51
because people's actual capabilities and invitations are situated within a particular workspace.
16:58
So that's why we actually have to be able to call free use of research,
17:05
where we basically go in somehow and try to understand for a very specific design situation what all users needs and wants, limitations, capabilities.
17:09
And so. Having done that. You cannot go to port four and we can reason about interaction.
17:19
So how people can interact with systems. And as it turns out, there is a whole range of theories of interaction.
17:27
Some of them are very basic. We are building on information theory and control theory, like what you are learning one bit.
17:33
Some of them are very mathematical. For instance, rational analysis.
17:39
We assume that humans are rational agents within certain resource bounds.
17:43
And some of them are really about using frameworks, for instance automation framework.
17:48
So how to think about automating or semi automating systems and what are human performance consequences of that.
17:53
Haven't understood the type of interaction we want to support.
18:00
We cannot go to quantify user interfaces. And now we can start building things.
18:04
And there are lots of ways of building things. We can build extended reality interfaces using glasses and so on.
18:08
We can give graphical user interfaces, command lines, prompt interfaces.
18:14
There's a lot of different ways of doing them.
18:18
Having done that, we should probably think about politics, decide how we actually go about in a, in a, in a, in a systematic way to design systems.
18:21
So what we mean by design by called design cognition like how humans design but also what other processes.
18:32
But people do much of the software engineering processes. The most common one is called usability engineering.
18:38
So we'll talk about that. That going from the design, we have to build things.
18:43
So this is for the best part seven engineering.
18:49
So this means for instance, how to map out systems, how to model safety and risk, how to build software and hardware.
18:51
But actually realise this can the solutions we have to sign and so on and so forth.
18:59
And finally perfect. How do we evaluate our system? And there is no perfect evaluation methods.
19:03
We have to use triangulation to combine different evaluation methods to make sense of what's going on.
19:09
So that's the high level fundamental structure. And what will people do when they try to do this is that we use this principle of being human centred.
19:14
Okay. So human centred means that we put the human first.
19:24
So the contrast is a focus primarily on the technology itself, which can be a valid stance, by the way, but it's not a stance that we would take you.
19:28
So human centeredness actually separates HCI from those of other disciplines.
19:38
And it has three key implications to be human.
19:42
First, we need to understand users, including their needs and motivations.
19:46
So it goes beyond understanding humans as a general construct to understanding how humans operate within a particular context.
19:50
Secondly, there is a requirement to engage with people.
19:57
Some of the research and design. So you don't design for people, but with people.
20:00
Um, thirdly,
20:06
there's a requirement for ethical consideration on how these kind of interactive systems will either directly or indirectly affect people.
20:07
So systems should match people rather than requiring people to match the system.
20:16
And we strive for the best understanding of people so we can come up with the best decisions.
20:21
And successful firms are extremely good at doing this, and this explains why we are so successful.
20:25
So we're going to engage with people that's part of and research and some.
20:32
And this implies a particular stance towards people. The primary rationale for any practical.
20:36
The same decision is based on an understanding when how based on the people, what we're going to be using will be affected by the system.
20:43
And then, of course, various other responsibilities,
20:51
either at home and find a best possible solution for people, which is true for engineering in general.
20:53
So to do this, it turns out you have to know a lot of things.
20:58
But you don't have to know things that deeply to be able to have a sense of what is going on.
21:01
But to give you a very simple example. This is a photograph of somebody sitting in front, uh, in a cafe or something like that.
21:07
And they had a mobile phone. We have an article. I don't know if the picture is big enough for you to see, but they are using an IED to write some.
21:15
Okay. So what kind of theorist would we need to explain this sort of situation?
21:22
Well, one of the basic things starting at the top left is what's called the pragmatic and the hedonism.
21:27
Right. So utility right. Doing useful things is called pragmatic interaction.
21:33
And doing things for pleasure is called hedonism. And in reality we do a combination of both.
21:38
Then there is a theory called self-determination theory.
21:44
But if we were talking about labour, which basically explains human beings, right.
21:47
So it turns out humans are not empty vessels.
21:52
We'd like an empty hole in our soul that we need to fill with stuff that's like a 19th century kind of way of viewing things.
21:55
Instead, people have a have a need to feel autonomy, relatedness and social relatedness and so and mastery of our skills.
22:01
So self-determination theory explains why we are motivated to do anything.
22:09
Then we. You're looking at the phone number, looking at the laptop. We have to multitask.
22:14
So there are multiple resources theory that can basically model and predict how we use these different kind of devices.
22:18
If we are moving on a touchpad that's a motor control action of fingers.
22:25
Moving on the touchpad to move a cursor on the screen. That can be a model with first order lag or a linear control system.
22:28
Now that particular irregularity is known as a feedback loop.
22:35
When we are using these solar systems, you have a little sensors in them and they can subtly affect the way they communicate.
22:40
Depending on how effective the state of actually uses what's called social information processing theory to explain that.
22:46
If you have multiple collaborators working in ID,
22:53
then we are working together with other people and we need to understand coordination and collaboration.
22:56
When we are searching for information,
23:03
we use what's called information foraging theory to mathematically predict where people are searching patches of patches over and over display.
23:04
You look to find information, but it's important to them in.
23:12
Then we try to carry out tasks, or we can analyse these tasks using what's known as task analysis.
23:16
I'm not sure two ways to do that. And finally, what we actually see on the display can be predicted based on visual saliency models.
23:22
So visually salient information is standing out and it's more likely to grab our attention.
23:30
As you can see, a very simple ordinary scenario. Whereas a lot of theories now, there's a lot of stuff going on to explain what is actually happening.
23:35
And it's all about fully understand what is actually happening. It's possible to build the best possible user interface.
23:43
Okay. So what is interaction. So interaction is basically an agent interacting or communicating with another agent and receiving responses.
23:52
Basically the important thing with interaction is that you can't design interaction.
24:02
Interaction is an emergent quality of a system, right?
24:06
There is no interaction when you build.
24:10
This interaction only emerges when what you're built has been shipped and people are using it to solve their problems in a particular context.
24:12
At that point, interaction emerges.
24:19
Whether that is good interaction with bad interaction depends on how good you are building the system to begin with.
24:22
So you need to predict what kind of interaction will emerge from your system in a variety of context,
24:27
by a variety of people you have never met, a context that you have never seen.
24:34
And of course, interaction often involves adaptation and even colour adaptation.
24:41
We learn, right? So humans learn and adapt to systems, and systems can also model humans and adapt to people as well.
24:46
So a user interface is basically about the input output of the system, what we see and how we can manipulate it.
24:57
And thus been a lot of design, but it's about the user interface.
25:05
But design goes beyond user interfaces to workflows and values and so on.
25:10
And user interfaces have changed a lot. They used to be punch cards.
25:14
I don't know if you've seen them.
25:18
We have a computer museum in Cambridge is quite good to go there once, or at least and see what people used to work with.
25:20
And there used to be a lot of command line interfaces in the 1970s and 1980s,
25:26
and there was lots of research about how we know how to name commands and how to best remember, remember commands and so on.
25:30
Basically, in the 1980s, we got the graphical user interface to work with.
25:37
And basically, despite all of his development, research and user interface, can you can learn a lot from previous work.
25:41
So one example I have here is unspoken dialogue systems.
25:50
So they actually learn a lot of a lot of how to create spoken dialogue systems based on command line interfaces.
25:53
I know it is when we have prompts. Prompts are basically command line interfaces as well.
25:59
And there's a lot of research and limitations on properties of command line interfaces.
26:03
But ultimately today. So here are some examples.
26:07
The figure here to the top left centre arm. And that's a real system is the first commercial virtual reality system.
26:11
You basically have this guy in a suit here and his sticking his head into read books.
26:18
That books has a display. It has a stereo speakers.
26:22
It has actually a fan. So it can blow air towards you from there.
26:29
And it can even generate smells like smell of gasoline and various things.
26:34
He's holding on to his hand. That's where of his hands would rumble. And a common simulation for scents.
26:38
Aroma was to walk in Brooklyn, in New York on a very warm summer day.
26:43
And you smell the gasoline fumes. You get the hot air blowing into your face.
26:48
And quite a compelling simulation of that was in 1962.
26:52
And then there is this classic system of pork, right, with sketchpad.
26:57
It's one of the earliest examples of a graphical user interface.
27:01
He directs the light pen, where basically you travel to see a key display.
27:04
You can create geometric figures, some manipulating and then hypertext editing, basically the way you click a word.
27:08
I went with the first system that was widely available for the hypertext editing system, and there's a screenshot here from 1969.
27:14
Again, it's a light pen, and it allows you to click when underlined words to go navigate in a in in the document space,
27:22
just like they would work with. And this is the first commercial graphical user interface.
27:29
Zero saw that is was presented at the 1980 1981.
27:34
And as you can see from this screenshot, this graphical user interface looks pretty similar to how they look today.
27:40
That's black and white, but the kids are still the same.
27:45
So another thing we have to think about is design. So what is design?
27:51
Well, we need a plan. We need a specification. We need to come up with some sort of prototype or system or a service.
27:55
Maybe some design, some artefact. Right.
28:00
So the site can evolve a user interface and relevant parts of the different underpinning system, but it can also be a service.
28:04
The important thing about design is design is really to think about how things could be right.
28:11
So in the future, if things fall right, well, different ways the future can take, right?
28:17
And as a designer, it's really about when you build something,
28:22
you change the future because you change the way people do the work and go about their work and do things in order to achieve their goals.
28:25
So since you changed the future design, good design is really about predicting what would happen in the future if I made that design.
28:33
And that's something designers think a lot about.
28:42
So design is really about being a magician in some sense, imagining how things could be in the world.
28:45
Engineering you heard a lot about. It's basically building things, realising how to best build something that meets a set of expectations,
28:53
but also emergent qualities that are important to people performance, safety, usability, interaction, as I mentioned, explainability and so on.
29:02
And the challenge of building systems is to anticipate all of these emerging qualities.
29:12
There's not much else I have to say here. Uh oh yeah.
29:19
Verse one interesting thing, actually.
29:23
Basically, one thing I was fond of very early in the 80s is that we need to assume humans are capable of creating errors.
29:28
Okay, we'll talk about this later when we talk about safety and risk. People tend to yield errors, right?
29:34
I have people misunderstand tasks, right? They have the wrong intention.
29:40
That's called a mistake. Well, we accidentally hit the wrong button, for instance, that's called a slip.
29:44
And we need to build systems that are error tolerant. And this leads to this idea of undo.
29:49
So, you know, undo in Microsoft Word and so on.
29:55
In the early 1980s, people were like researching one of the best ways to modulate, undo, believe it or not, right now it's a sort of problem.
29:57
But it wasn't a soul problem back then. And having built something we need to evaluate.
30:04
So we need to use some systematic methodology to convince ourselves our system is fit for purpose.
30:10
And there are lots of things we can look at. We can look at performance.
30:16
We can look at people's experience. We can look at safety.
30:19
It depends on what is important to us in general. We need to figure out a way to verify.
30:23
So we need to verify that the design meets the requirements.
30:27
We have come up with an important thing we need to validate, which means our system is actually fit for its intended purpose.
30:30
It's very possible to build a system according to requirements that succeeds verification but fails validation.
30:37
For instance, if we build a system but it's a solution in search of a problem,
30:43
then that system might work very well, but nobody's going to use it, right.
30:47
And there's lots of examples of that. And we also have to do what people call testing, for instance, usability testing.
30:51
This means we are basically trying something out under realistic or semi realistic settings,
30:57
as well as what we see when we talk about software engineering models like usability engineering testing is an integral part of that.
31:03
Importantly, as I said in the very beginning, people use the principle of triangulation.
31:12
So you need to use different methods to properly evaluate a system.
31:16
So we need to arrive at robust UN general about likeable and reproducible findings.
31:20
But practical design decisions can actually be based.
31:25
So it's very naive to think that a single evaluation or limited valuation can actually give us confidence that the system is fit fit for purpose.
31:29
So in practice, people use a variety of methods. For instance, they combine interviews with ethnographic studies, will be combined,
31:37
experimental control studies with interviews, and so on to try to tease out all the positive and negative aspects of a design.
31:44
In practice, evaluation is also closely connected to the principle of iteration, right?
31:53
So you build a system, you test it, you learn things, and you improve the system, and you keep iterating a couple of times.
31:58
And in companies people do that a lot. You don't see it as consumers.
32:04
But if you work within corporate firms, you'll see this kind of principle of iteration happening a lot.
32:08
Okay. Why does it matter? First of all, interactive systems tend to be difficult to use.
32:17
So the discipline was created out of the human factors discipline because people simply couldn't use systems.
32:23
They were completely unusable. Same was true with programming languages, by the way.
32:28
They were incredibly obtuse in the 1950s and 1960s.
32:33
So as interact a system begins to used by non-experts right to work specific specifically trained to use a specific system.
32:37
That's a real problem in how do I actually provide input to the system and how do I interpret the output?
32:44
Um, we have probably all used systems that are difficult to understand,
32:51
difficult to understand instructions from requiring us to work around stumbling blocks.
32:55
Or maybe we gave up altogether. And actually, there is a lot of problems with interactive systems.
32:59
So typical things people find is that about half the time people spend on a computer.
33:06
It's basic because people are fiddling around trying to get things done right, and these challenges seem minor, but they compound right.
33:11
So for big organisations, a lot of time is lost for these kind of poorly designed interfaces.
33:19
Here's one example. So forget about user interfaces.
33:30
So think about a very basic interface a door. Have you thought about how complicated a door is?
33:34
Right. We have the door itself. You have hinges. You have a door handle, right?
33:40
So there are some systems within the system. A door has load bearing functions, right?
33:43
It can also have implications for health and safety. Like it could be a fire door for instance.
33:48
It controls the flow of light. To build a door.
33:53
You have to basically collaborate with people making hand door handles, hinges, screws and all of this stuff.
33:56
There are regulations about doors and that's just a door, right?
34:02
And then you see all these doors that are super difficult. So this department is packed with these kind of doors, right?
34:06
No cooler door push. If you go to the Baker building a third floor and you try to enter the single processing lab.
34:12
There are two buttons and it says, do not press this button. And then for other one person I was saying, do press this button.
34:19
Right? This kind of stuff, right? And that's just a door. It's not even a small door.
34:27
Right. So things tend to be difficult to use.
34:30
It's not that people try to build up two systems, but if you don't carefully think things through, this is bound to happen, right?
34:34
It's inevitable. And one reason for this is what people call the egocentric fallacy.
34:41
Basically, we all think we are the most important person in the world. Unfortunately, we are not.
34:46
And we think that everybody is like us, right? But if you basically decide based on intuition, what does that actually mean?
34:51
Your intuition is your experience, right? If you decide based on intuition, it's your experience that you're designing for.
34:59
So the egocentric fallacy is to assume everybody is like us, right?
35:07
To explain how everybody's behaving in terms of us, which is a very human things to do.
35:12
We ascribe humanity to everything. Cats, dogs, cartoon characters, cuddly toys, etc., right?
35:17
It's something humans do, but you are often not the user.
35:23
Right. So that's the one. So intuition can only go a certain distance.
35:26
And it's often downright misleading. And there's lots of research about this.
35:31
There's a comedy paper in actually transactions software engineering from horror film bully about calculators.
35:36
Did you know about every single commercial release? Calculator has this form on every student and not a single commercial release.
35:42
Calculator is internally consistent. Right.
35:49
This can be formally proved actually by just very simple operations.
35:52
So calculators, for instance, have a lot of assumptions like whose example of press clear, right.
35:58
Multiple times. So you've seen the calculator you know about calculators.
36:03
Now that's just a calculator. Another reason why we need.
36:06
Why we need to do things is for it. It's the right thing to do. So being human centre is a value of its own.
36:11
Because you can. You can make things better, right? You can create interfaces that have less frustration and allow people to do more.
36:17
Things are amplify human ability and improve well-being.
36:23
There is a classic book by Thomas Landauer from 1995, The Problem with Computers,
36:27
where he basically goes through a whole range of usability problems that prevent computer uptake.
36:32
The funny thing, that book was published in 1995 as A Long Time Ago.
36:37
All the things you list in that book are still true. We just sort of learn to cope with it, right?
36:41
So we have a duty to improve computer use. The other reason to do it is you can make a lot of money.
36:48
So there's been lots of studies about this. And again big firms understand this.
36:53
Investments in human computer interaction has a huge payoff. You increase productivity, you find new markets and you lower costs.
36:57
And basically you can you get a huge rate of return on improved usability.
37:05
Now there's a classic cost, uh, usability cost justifying study, uh, from 1992 that shows basically how well you can actually make money.
37:12
And also most of the software code relates to the user interface.
37:23
Another more exciting reason is that you can invent the future so it's easier to complain about things,
37:27
but you want to be a part of the problem, or you want to be part of the solution. So you can actually create new systems and services.
37:31
As we know, for the fear of product lifecycles. People come up with a system, you get some initial success.
37:38
Hopefully eventually your system will decline and there will be new things, right?
37:43
So you need to constantly reinvent the future and you can be part of that.
37:47
You can discover new ways to use products, services and so on.
37:51
And we see this commercially all the time. In fact, there are two specific ways you can do this.
37:55
You can use the mechanism of market pool, which means you go out, you do your user research.
38:00
You understand how organisations or consumers think and behave and understand the problems,
38:05
and you find a gap if you find a need that you want to fulfil, right.
38:09
So that's market pool and that can be extremely profitable.
38:13
The other way to do things is to take the design stance, I think, is where some ingenious new system and service I can build,
38:17
but I think it's really, really good and I can convince people to do it.
38:24
That's cool. Technology push in.
38:27
In practice, people use a combination of market pool and technology push to realise new technologies, services and systems.
38:30
Can we predict the future? Well, there are some examples, but I think sort of says that sometimes we can.
38:39
One of these is Douglas Nagel later received a Turing Award.
38:45
When he created what's called the mother of all demos. If you Google mother or demos or go to YouTube, you can find this video.
38:48
Don't look at it. No. And basically it demonstrates essentially a graphical user interface, right as we see them today.
38:55
But in 1968 and it shows a terrible computer mouse based on four different wheels.
39:02
Not a very good computer mouse, but it's one of the first computer mice people have done.
39:08
It totally predicted the graphical user interface we still use today.
39:13
Another thing is we can see all these tablets. What people have, right?
39:17
Is tablets a new thing? No. So in 1977.
39:20
Allocating another Turing Award, created a diet book which looks something like that,
39:25
and it's basically a tablet you could carry with you to create a computation on the fly.
39:30
It was actually designed as a teaching tool for kids,
39:35
so people could basically interact graphically with stuff and types of things and so on were ever built.
39:38
Right. And that was in 1977. Right.
39:43
So it is possible to predict the future from a variety of angles.
39:47
Okay, so how do we do this? Well, the only way to do this without just going totally random is to use principles and skills.
39:51
Okay. So there's a whole set of principles that people have discovered.
40:00
And knowing these principles helps you building things. So what is a principle?
40:03
It's some sort of foundational ID, or a rule that can help us predict how to enable control of a human computer interface.
40:09
So, for instance, one principle we will learn about is the principle of direct manipulation, which is very famous.
40:18
The term direct manipulation was coined by Ben Schneiderman in 1983.
40:23
And it basically says that computational objects, that software objects should be made visible to people,
40:28
and it should allow people to gradually interact with them in reversible ways.
40:34
So an example of this principle is a file. If you have an icon of a file in a desktop graphical user interface.
40:38
But I know that's fine. It's a visualisation of the software object, the actual five, which is actually stored in the storage system of the computer.
40:46
You can click on that file, but you can double click typically to open it up.
40:54
You can right click you to see the commands you can apply on that file.
40:59
You can click that file and drag it to the trash can to delete it.
41:02
That's an example of the principle of direct manipulation.
41:06
And as we will see later, there are some very strong empirical and theoretical foundations for why this principle is actually incredibly helpful.
41:09
So these kind of principles is what we will use to basically guide us.
41:17
Another thing is that we have a pluralism in Memphis and theories.
41:22
Basically, it's not possible to use a particular approach or stance to fully explain what's going on.
41:26
So we need theoretical pluralism. So basically it's been almost an eye to eye movements, emotional reactions, aesthetic experiences,
41:32
social interaction, uh, organisational structures and things like motor control.
41:42
And so when I so forth, uh, we need to understand, uh, have a, have understanding of all of these aspects in order to actually build systems.
41:46
What works. And insights are supported by research.
41:54
Right. So instead of just thinking about fats and stuff like that, we're based upon empirical findings.
42:01
Right. So there is a lot of stable, robust empirical findings that have been replicated many times over.
42:09
And we know the extent to which we generalise.
42:15
And we will explain empirical findings relating to principles and methods and insights and so on and so forth.
42:17
And there is an optimism that things are indeed solvable.
42:27
We can do things right. So a lot of systems and I will show you another slide by Ben Schneiderman later actually built on HCI theory.
42:30
A lot of key systems, e-commerce systems like Amazon and so on, social media, augmented reality, you know,
42:38
across device and so on and so forth on how we do text input and editing and undo actions in an interface are all based on HCI research, right?
42:45
And actually for a lot of problems that we are facing there all solutions, right.
42:55
But people have to know these solutions. So frequently we do have the answers, but we need to know them.
43:00
Here is a slide about the impact of HCI. It's by Ben Schneiderman.
43:07
And it basically shows how HCI theory applied to Amazon, eBay, Samsung, Apple Microsoft, Yahoo, Google, Twitter, Facebook, Nintendo, Electronic Arts.
43:12
And this is a couple of years ago. And it can be updated not once on what's happening.
43:21
Actually, it was generated by HCI research in academia and in industry.
43:26
Okay, now I'm going to spend the last ten minutes giving you a couple of examples.
43:32
So here's an example on motor control. So this guy is sitting in front of a very high fidelity AI tracker that can actually
43:38
track tracked the eyeball movement and user's gaze location on a screen to a very,
43:46
very high degree.
43:51
And basically what we're interested in here is do eye movements actually follow, uh, with motor control regularity known as Fitts law?
43:53
So Fitts law basically says that the movement time it takes to move a mouse cursor from one location to another is related to,
44:01
near, or related to what's called the index of difficulty.
44:09
The index of difficulty is essentially how big the target is for the target is away, and it has a long relationship.
44:13
It's we will see it later when we talk about control theory, but it arises directly from a linear control system.
44:20
And what's interesting is that when you move your hand to basically steer a cursor or you touch on a touchpad to hit the targets,
44:27
that's very different from how the eyes work. The eyes are primarily a sensor organ, not a controller, right?
44:34
So the way the eyes move is that you fixate. So you fix it up at a location.
44:41
And then because of the muscles tend to reverberate, there's basically oscillation.
44:46
Right. But basically your muscles tend to stabilise your eyeballs so you can fixate on specific location.
44:50
Then you fix it for a set amount of time, because it's actually very fatiguing to fixate on a location for a prolonged time,
44:55
and then you're able moves a ballistic movements to some out of target.
45:02
Right. So you have this fixations and these ballistic movements called circuits.
45:06
So it's not clear whether eye movements actually would follow this regularity.
45:11
Fitts law. Now what they showed here is that indeed it does, with some caveats that are pointed out in the paper.
45:15
Now, a totally different thing. This is something I heard about when I was at IBM devoted to fossils attachment.
45:22
So basically lots of big firms built tools for system administrators.
45:28
We all service companies. System administrators do a ton of stuff.
45:32
And in order to retain customers, you need to understand what the system administrators is doing.
45:36
So what they did is they did an ethnographic study of actually understanding what all of these system administrators are actually doing.
45:41
What they tend to be doing is they tend to tailor and tinker and create their own tools and modify their own tools.
45:48
And they thought about all the existing systems that are supplied commercially. We're actually not really suited for this tinkering and tailoring.
45:54
So based on this insight, they made it tools more manipulable by system administrators, hence making them more happy customers.
46:00
Here's another completely different example. So we all have different abilities.
46:11
Some people have been severely limitations in terms of their own motor control for a variety of reasons.
46:15
Maybe they suffer from ALS or CP. If you suffer from ALS, you're probably going to be a struggle to actually move your muscles.
46:20
Mileage loss, muscle force. That means you're going to move very, very slowly.
46:28
So you want all the user interface widgets to be very close together.
46:33
If you suffer something like CP, you can probably make rapid movements.
46:37
But these rapid movements would be very imprecise. So you want a bigger interface on wheels.
46:41
So what they did here is that they basically went through a series of talks to basically train a statistical model on a user's motor control ability.
46:46
And based on this model, they did a rather simple device to automatically generate interfaces based on people's motor abilities.
46:54
You can also come up with just crazy stuff. This is from Disney Research a couple of years ago.
47:06
Well, 13 years ago. Time goes quickly. So this new research is basically, uh, Disney is a theatre company, right?
47:10
So they want to create experiences for people. So this new research was investigating like how can we create completely new experiences.
47:17
So one thing we did is this interaction with trying to persuade directors.
47:24
You can basically touch different plans.
47:28
And based on when it comes to plastic, identify the location where you touch the plants and you can do things with it.
47:31
So you can do either real plants or artificial plants. And electrically, this is actually quite a simple idea.
47:37
You just excite the plant by a signal path, but it's varying in its frequency.
47:42
And because plants have different conductive pathways, when you touch the plant exchange, the pulse rate is very easy.
47:47
Using stand up machine learning to figure out where you touched a plant and suddenly you can interact with plants.
47:53
You can touch, touch the plant, the certain locations and things can happen.
47:58
So you can create new things that people didn't actually with what we didn't knew existed before.
48:02
Another thing people can do is they can basically figure out things about the sun.
48:08
So this is one example of what's called value, uh, value sensitive to sign off.
48:13
In this case, AI algorithms versus about weaker products, which is supporting Wikipedia and all of these things.
48:19
They basically wanted to have an AI algorithm to support people identifying edges on how to go about work,
48:25
but they also wanted people to have a big acceptance of how this algorithm work.
48:30
So they work with people to actually understand, uh,
48:34
how people felt about this algorithm and how they interpreted the consequences of this algorithm and fed up into the the sun.
48:37
So this is known as value sensitive design. Uh, here's an interesting one.
48:43
So this paper here from Houston, it's 1999, essentially created a new field called and usable Security.
48:50
So security is a big thing. And it's been a big thing for a long time.
48:57
And as you all know, you can encrypt mix.
49:01
And so what these studies show, which is the first of its kind, is that even if you build an encryption software.
49:02
So this was studying PGP Pretty Good Privacy, which had a UI and graphical user interface built, you know, design based on the guidelines at the time.
49:10
And even if you build that user interface based on all the use of the accepted guidelines at the time,
49:19
you can still end up with an unusable security system.
49:26
So they had quite a student. So these were quite, you know, well-trained students understanding technology.
49:30
And we asked them to use PGD to encrypt the image of sending the image to their friends.
49:35
Use of this graphical user interface. Facebook. All good principles at the time.
49:40
So there was a number of problems when you did this.
49:46
Uh, only one third of the participants were able to correctly sign and encrypt an email to begin with.
49:49
So two first totally failed. And 25% of the people actually sent a secret key to the participants.
49:54
Life is totally within the first idea of security in the first place, and what they could show is for just creating a user interface, right?
50:01
Based on standard guidance, is insufficient to deal with usable security.
50:10
So this led to the field usable security. And there's been lots of interesting follow up studies after that.
50:15
Here's another example. So this is an augmented and alternative communication device.
50:23
So imagine an alternative communication is used for non-speaking individuals with severe motor disabilities.
50:29
And depending on the severity of the motor disabilities and any potential intellectual disabilities, you may have to use this kind of isolated system.
50:35
So what you see here is a bunch of iPhones that you can click these icons to basically let the system speak out sentences for you.
50:42
The problem with icon based systems is if you just do a simple mathematical calculation, you can only fit so many icons on an iPod, correct?
50:49
So what we do here is allow people to take a picture and based on the picture of their surroundings,
50:57
it will automatically populate the icons suitable for that picture.
51:02
But that's not what I want to talk about. That's just what the system does in order to actually investigate.
51:07
So we actually have to basically work with Arc professionals.
51:12
So people who work with people with severe disabilities and they are hard to find and they are professionals.
51:15
So what we did here is we actually have releases on the App Store and through the app.
51:21
We were doing remote interviewing with these AC professionals to validate how the study was done,
51:25
so they could actually evaluate this system with professionals in the field, across the world, through the app itself.
51:30
So these allow me to carry out in-depth interviews with a whole range of ASC professionals across the world,
51:37
instead of just talking with a local AC professional, which would want to give you a very limited view.
51:42
So what is the summer here? Computers are pretty powerful.
51:49
I mean, I think we can all agree on that. And interactive systems and in particular user interfaces has this statement, but allow us to control it.
51:52
Human computer interaction is concerned with people. It's about creating technology and understanding interaction.
52:01
So what we need to do is to understand people studying what people need or want,
52:07
and then design engineer systems that actually work and of fit for purpose and evaluating the benefits for users.
52:11
A very important thing to remember is this egocentric fallacy.
52:18
You are not the user, right? You learn a lot by talking to other people.
52:21
But as we will see later, just talking to other people is also very dangerous because people like this call to say do problems.
52:26
People say one thing and do something else. So it's not that simple.
52:33
But you should remember you are not the user.
52:37
So overall, it's a disciplined approach to basically tackle all of these challenges, but tends to leave demonstrably to much better systems.
52:39
So it's quite exciting. And here are some reflective exercises you can do after the lecture.
52:47
And if you want to read more this textbook is completely free open access.
52:54
You don't have to pay a penny. I have been talking about chapter one Introduction to Human Computer Interaction.
52:59
And I will see you again in the next lecture where we are talking about understanding people.

Lecture 2
All right. Welcome to forum 21. Software engineering and design.
5:05
Uh, in the first lecture, we talked about an importance in understanding humans, which is like generalisable knowledge about humans.
5:10
And then thereafter, we have to find specialised knowledge, right, about how humans want to need to use technology in particular context.
5:17
So today we're going to talk about the second part, which is uh, yeah.
5:25
Or sorry, the first part, which is basically about understanding people in general.
5:30
So properties of humans uh, in general.
5:34
So we're going to do a whirlwind tour for a number of different ways or facets to look at this.
5:38
We can have a motivating example. We have two scenarios here.
5:44
We have somebody playing some sort of game, uh, together with other people,
5:48
and we have somebody driving a car and trying to fill it with a phone at the same time.
5:52
Right. Both of them involve sharing, uh, affect, uh, perception because, uh, for, for the, uh, woman to the right,
5:56
you have to share your attention and your perception between the phone and the driving environment.
6:09
And for that screen over there, it's actually quite cluttered. Right.
6:14
And as we would say for perception, perception is a lot about attention, right?
6:17
If you can't attend to something, you cannot perceive it. So we both are affected by perception but are also affected by motor control.
6:21
Right. So for many games you need to be very fast. So you need to move your mouse pointer or whatever.
6:29
Uh, very quickly and accurately. For instance, able to intercept the fast moving target that can be small and move in difficult ways.
6:34
And if you have, if you have, uh, if you're driving, but also obviously you have a steering task,
6:42
but you also have to monitor, you know, how you actually hold, uh, your mobile phone.
6:47
In terms of thinking.
6:54
If you're playing the game, you have to keep track of several events the current status of the game, what everybody else is doing.
6:55
You need to build what's called situational awareness of what is happening in the game.
7:00
And if you're driving, you also have to build situational awareness,
7:04
because you have to have a model of what is happening in the road in front of you. And if you want to interact with the phone at the same time,
7:07
you need to know what you need to do to actually interact with your phone and proceed to the next stage.
7:12
And then there are something very slightly beyond the nitty gritties of perception and motor control and also needs.
7:20
Right. So what are our desires and what are our perceived risks in our interaction.
7:25
Right. So we have different needs. For instance why do you play a game.
7:31
Right. Some people don't play games, right?
7:34
What is the need that is fulfilled by playing the game? And do people play games for different reasons than using social media, for instance?
7:38
Maybe not actually, because there's only a couple of basic human needs that can be fulfilled, right?
7:48
So those basic psychological needs might actually be the same.
7:53
They just fulfilled with different kind of ways of interacting. Then we have issues of experience so you can be immersed in a game.
7:57
You're not really immersed in mobile interaction usually.
8:05
So when it comes to when it comes to a mobile scenario, for instance, it's more about being satisfied, right?
8:09
Satisfied with completing different tasks. And we can also have issues about communication.
8:15
So in games you have to communicate, coordinate and collaborate and communicate with people in various ways.
8:22
It may require intense communication or impromptu collaboration in a driving scenario.
8:28
You can do communication while driving, which forces you to multitask, which affects the way you are going to communicate.
8:34
And that leads to these different areas we're going to go through today. The first thing we're going to talk about is perception.
8:41
So perception I think is quite an obvious thing. Perception is how we perceive the world okay.
8:47
So perception has to be actively managed.
8:54
So what is happening for instance. And there's a famous research paper about this.
8:58
If you engage with a mobile phone and you're walking around in a busy street.
9:01
What's going to happen is you're going to divide your attention in a very predictable way between your mobile phone and your surroundings,
9:05
which can be very accurately model. And if there's one thing you need to know about perception, it is this little graph here.
9:11
Okay. So perception is the outcome of sensory information.
9:17
Like you need sensory information to perceive anything at all. But that is like the bare minimum.
9:22
The real necessity when it comes to perception is that you need to expect things to happen.
9:26
Okay, so you have a model in your brain,
9:31
but it's basically generating and building a world model for you in your brain about how the world actually looks.
9:34
It's not how the world looks or behaves, actually, but you think it is because it's your world model.
9:40
And based of this model in your brain, the brain is expecting things to happen, right?
9:45
And if your brain is not expecting things to happen, you will not perceive.
9:51
Right? So it's not enough with sensory information. You also have to expect things to be there in order to proceed.
9:55
But there's also not enough because even if sensory information, you expect things to happen, you also have to pay attention to.
10:01
So you only get perception. You can only perceive something if there is expectation, attention and sensory information.
10:08
If these all three things are not in place, you will not perceive anything.
10:16
So what is sensation and transduction? So sensation is a physiological process, right?
10:21
It produces information about the general environment that we can sense.
10:26
Sensation is required to form what's called an integrated and actionable view, something the brain can actually work with on recent weight.
10:31
And sensation feeds perception. Right? So again sensory information is just one aspect of perception but necessary.
10:39
And sensation is really transduction. So you can transact a variety of input signals into something we can sense.
10:46
So you have mechanical sensitivity, chemical sensitivity photosensitivity and so on and so forth.
10:53
So the typical modalities we do care about, there are more modalities,
11:02
but we tend not to be back and forth in user interfaces is a vision which has very fast and has a very high bandwidth hearing, which is very fast.
11:05
But more cereal. Right. So hearing tends to be buffering cereal and temptation.
11:15
Right. Basically in a sense, the tactile sensation you get when you stroke your fingers over a hot surface, for instance.
11:20
Right. Which is very, very fast actually, but is limited because of the physical contact.
11:26
Right. And then we have these ideas of perception.
11:31
So one of them is the windows of visibility. So that's one of the visible spectrum of light that we can see.
11:36
Obviously that's a wider spectrum we can't see. We also limited our field of view.
11:41
So our field of view is about 190 degrees. Beyond that we would not perceive anything.
11:45
We also have limitations in terms of contrast. So our perception of detail is limited.
11:50
And contrast sensitivity basically is the term for how much contrast we can actually sense.
11:56
And we have this idea of what's called foveated vision. So the retinal image is very limited and very non-uniform.
12:01
So what is actually provided to the brain is actually quite messy. And it loses a lot of accuracy at the periphery.
12:07
But we can still reconstructed because the brain has a really good word model about what is happening and filling in the blanks.
12:13
So windows of disability can be used for lots of things. So for instance, if you're glancing at a smartwatch for tying your shoelaces,
12:24
you can think about the windows of visibility, what you're actually receiving here.
12:31
So this is something you can think about after the lecture. Okay.
12:35
So one thing we can do with perception is we can model eye movements.
12:39
And when it comes to eye movements there are basically two things that happen. So one is that we engage in fixations.
12:42
So fixations means that we basically rotate the eyeball and the muscles below the eyeball in place,
12:47
where we fix it at a particular location in the screen. Right.
12:53
And then we move the eyes move in a ballistic fashion, as I told you in the last lecture.
12:56
And these are called skates, right.
13:01
So you're basically going to fixate and it's going to vibrate a little bit because it's very difficult to keep the eyeball fixated.
13:03
And then we're going to move in a ballistic movements to different fixations. And a series of fixations forms what people call a scatterplot.
13:09
Now this campus can be used for a variety of things. One thing a scan path can be used for is what's called smooth pursuits.
13:17
So this means smooth tracking of moving targets basically without saccades.
13:24
So basically the eyes are going to follow a continuous tracking in a continuous tracking manner.
13:28
So you have fixations, saccades and smooth pursuit.
13:35
And this can be exploited in various ways. For instance, what we did here is that you can look at your smartwatch and it actually has an eye tracker.
13:39
And if you follow that red cursor but moves in a circular fashion right to the right ear, to the left,
13:47
assistant cannot dramatically detect if you want to increase the volume or decrease the volume or whatever you want to do.
13:52
So it's an exploitation of something that is fundamentally human. Another thing we have to think about is perceptual organisation.
13:58
So if you look at your desktop screen, you may see something like this.
14:05
And you would take for granted. But this little pane here right, or window is on top of the background.
14:09
Right. But is that obvious. What is figure on what is ground.
14:16
So in this example, this pane, what we feed here and the backdrop image here would be ground right.
14:19
And this is something our human perception does. The brain models the world in such a way.
14:26
But it separates out foreground figure from background.
14:29
And if you think about it, there are lots of visual group in some of these play with this drawing.
14:34
We have a little panel here. You have the window and you have the background, etc.
14:39
So basically the way we do this is called perceptual organisation.
14:45
So we have figure ground perception which explain we have regions that can be decomposed into separate regions.
14:49
And we have what's known as visual groupings. So we will automatically group things together based on how things are distributed visually.
14:55
So here's a very simple example of that. We have a bunch of application names to the left for Ungrouped.
15:03
There's one group. If I split them by proximity, you can see that you perceive them as two different groups.
15:10
If I split from a common area, right. So I draw a rectangle around them or some sort of delimiter, they will perceive the different groups.
15:17
But I can also split into different groups by similarity.
15:25
Right? I can change one quality of the text here and they stand out as two different groups white red versus black.
15:28
And I can also introduce a discontinuity. And I can draw a line and we perceive it automatically as two groups.
15:36
If you think about it from a purely visual sensory information point, there is no reason this should actually result into groupings.
15:42
But it does because of the way we perceive, because the way the brain does.
15:49
Perceptual organisation. So perceptual organisation is important.
15:54
But what I also said in the beginning, if you perceive you have to pay attention. So if you don't pay attention you can't perceive anything.
16:00
And there are two particular effects that arise due to this lack of visual attention.
16:07
One is called change blind blindness,
16:12
which is basically the failure to detect a change if there is some sort of discontinuity in the perceptive field.
16:15
For instance, if I have an iPhone here.
16:21
And I have icons on the screen and I remove an icon.
16:26
If you close here, you will probably see the icon being removed. You would notice something changed, right?
16:30
If I show you the icons and I blank screen. So I show a complete black bitmap over the entire screen and I remove the icon.
16:35
Then. And then I'm black the screen. It would be much harder for you to detect the icon, but change the was removed.
16:42
And that's change blindness. But disruption basically causes change blindness.
16:48
Another failure we have is inattentional blindness.
16:53
We pay attention to certain things, and because we do that, we don't pay attention to other things.
16:56
And that's an evolutionary trait. If we paid attention to everything that moved, then we would pay attention to every leaf rattling in a bush,
17:01
for instance, which is not very good use of computational resources.
17:08
So there's a very famous paper called Gorilla Genomics where we asked, well, people like you actually grad students,
17:12
so many students to basically sit and look at the video and individual were asked to perform a task.
17:17
People were basically shuffling around papers or whatever, right, and count the number of times to shuffle the paper.
17:23
And in the back from a graduate student dressed up as a gorilla and walked in the background.
17:28
And then they asked everybody from the top, do you see anything unusual in the image?
17:32
Nope. Because they weren't looking at the gorilla, right?
17:36
They weren't expecting it and they weren't paying attention to it. So another thing we have is visual saliency.
17:40
So certain elements attract visual attention.
17:48
And if you use an eye tracker you get these kind of, uh, shots here, heatmaps here, but basically indicate what people tend to look at.
17:51
And this saliency is based on the distribution of visual features.
17:59
So some visual features stand out. More attract attention. But it's also based on expectations.
18:03
What we have learned to look at. For instance, many eye tracking studies have confirmed that people are very good about looking at something positive.
18:08
You have the expectation. And it also depends on attentional strategies.
18:16
For instance, if I want to carry out a particular action I will look at certain tools right in order to basically carry out my goals.
18:21
So here's one example. What makes a display cluttered?
18:29
I would say this just the background is slightly cluttered. And then you might think, well, what is the definition of clutter?
18:32
Well, the definition of clutter can be if I would put some elements here, write some post-it notes or whatever,
18:39
right to stand out, what visual features would I use to make it stand out?
18:45
And if I can't think of any visual features to make it stand out, this display is de facto clutter.
18:50
So that's perception. Now let's look move into motor control.
18:56
So you already know about control. So it can be pretty quick here. Obviously motor control is essential in interacting with any sort of interface.
19:00
We need to move our hands fingers and eyes etc. and legs to basically carry out things.
19:07
And that's been a lot of research about motor control. There are lots of examples of motor control in HCI.
19:12
We can change graphical layouts to better suit people's and motor abilities.
19:20
We can move things around and optimise things, so it's faster for people to access information that they want to access more frequently.
19:25
You can do crazy things like changing keyboard layouts and stuff like that,
19:32
which takes a long time to learn, but I know eventually it will be slightly faster.
19:37
And what we can also do is what people like Apple do. For instance, right?
19:41
Whenever you create a new touchpad, etc., you actually measure it experimentally.
19:44
To understand the throughput of that input device. You can benchmark it against other input devices.
19:48
So what do we have when we talk about a motor task, we have what's called an end effector, the body part that actually is used to actuate something.
19:56
We have degrees of freedom. How many ways we can translate this end effector like my finger into 3D space.
20:03
And then we have two very, very important concepts.
20:09
So this is basically open loop and closed which comes from control theory literature actually cybernetics actually in the 60s,
20:12
if you want to do things fast you want to do things open loop.
20:20
If you want to do things slow and deliberate, but accurate. You want to do things closely.
20:25
So if I'm if I'm trying to have a phone and I have a finger and I want to hit the particular item, I'm going to use closed loop interaction.
20:30
I'm going to use visually guided motion and proprioception of my fingertip to ensure that finger hits that specific item.
20:38
Okay, if would be slow, but it would be pretty accurate.
20:45
Now let's say I'm typing in an ATM code, right? Blah blah blah blah blah.
20:50
Enter, right? I can do it really quickly, right?
20:54
You probably don't think about your ATM code if you're if you're even using an ATM nowadays when extracting money from a machine.
20:57
Right. That's because that's open loop interaction.
21:03
But you have built this mode of particles in your brain, and you articulate them directly from the memory.
21:06
And the right articulation for the memory is more noisy.
21:11
So it won't be as precise, but it will be very, very fast.
21:15
So doing a 3D gesture that thumbs up is very, very quick, but it won't be precise.
21:18
If I do it three times they would all be different, right? Doing things over.
21:23
Lucas is fast but noise, doing things closely, right?
21:27
Deliberately moving to finger to hit the particular location.
21:32
The screen is slow but accurate, so there is a trade off.
21:35
Another concept people talk about relate to. This is aimed a movement said attempt to move an end effector to a certain location.
21:41
We'll revisit that soon. You can also do what's called an intersection task.
21:47
So you have a task where basically you try to catch something that moves, right, for instance.
21:51
So this involves both temporal and spatial demands. And in general you have what's called a speed accuracy Trade-off.
21:56
Right. Higher speeds tend to result in lower accuracy and vice versa.
22:03
Now there is a very famous little regularity that kind of I feel obliged to tell you.
22:08
And that's known as Fitts law. It's a very simple thing.
22:14
The movement time to hit is, for instance, moving a mouse cursor and hitting an icon on the screen.
22:17
It's going to be related to a plus b times ID so I so a plus b are just regression coefficients.
22:23
Empirical parameters like that that depend on the particular task.
22:31
And id is known as the index of difficulty. And the index of difficulty is a construct.
22:34
It's basically measures how difficult it is to hit the target.
22:39
If you think about it, that would depend on the size of the target and how far away it is.
22:43
Obviously. Right. And it's good. And it turns out for lots of experimentation, I think the thumb is going to be no basis arbitrary,
22:47
but people use based to normally the distance to the target over the width of the target plus one.
22:54
And this fits low has turned out to be super generalisable.
23:01
It applies to, to uh, to uh, a finger touching a touchscreen, moving a mouse, uh, using a light pen on a CRT display.
23:05
People have even tested it under water and in space. It applies for a lot of things.
23:13
So this is an example of predicting armed movements. So this is the original task from Fitz in 1956 I believe.
23:20
But this is from that from a from a review paper 1982. The original paper is in the 1950s.
23:27
So basically what it did is that people who held pen, that's why we as a core were to the right.
23:32
And then you have these targets, you see these black stripes,
23:37
and basically you change the distance to the target and we're able to talk like a stopping distance.
23:41
And then you measure how quickly people can hit the targets going back and forth, back and forth under different conditions.
23:45
And if you do that and plot movement time as a function of the index of difficulty,
23:51
how big the targets are, the distance from the target, you get a line. Quite a regression line.
23:55
Then what people think is. Hmm. Well, the problem here is that you have a speed accuracy trade-off.
24:02
I can get really fast, but I can make lots of errors. So you want to quantify things.
24:07
You want a construct with just one dimension. And one of these construct people use is known as throughput.
24:11
And there are basically two different ways of defining throughput.
24:17
One is to take the average index of difficulty and divide that by the average movement time.
24:21
The downside is that we rely on some average index of difficulty, which may or may not make sense.
24:27
It makes sense for an iPhone because the icons are the same size.
24:32
It doesn't make sense for a computer game where you have targets of varying different sizes.
24:35
An alternative way to do it is simply look at the slope right, and look at the rates.
24:40
And the downside is that it ignores the intercept a okeydoke.
24:45
And these people use this to benchmark different devices.
24:50
Then what you can do instead of target acquisition is crossing.
24:54
So you target acquisition I have a target and I have to hit the target and I can't overshoot if I overshoot if I missed thought in crossing.
24:58
I just have to move my cursor or whatever it is and cross the top.
25:05
But I can go as long as I want in the other direction. Okay, so the difference between target acquisition and crossing is that you don't have to stop.
25:09
As it turns out, this follows the same mathematical regularities.
25:16
Fitts law. Exactly. For some reason. And then you can do steering.
25:19
So you can steer with a car freely, steering where you can steer a cursor or a fingertip through a tunnel without hitting the tunnel site.
25:25
Now that's known as steering. And steering is the average movement.
25:33
Time of steering is again going to be A plus B regression coefficients.
25:37
And then we can integrate to the tunnel. Right. Because the tunnel obviously can vary in its width.
25:40
And if you differentiate both sides of the equation here we find that basically
25:46
the instantaneous movement speed is proportional to the width of the tunnel,
25:51
as you would expect. What's kind of interesting with the steering logs.
25:54
So the index of difficulty I should say this first is basically the, the it's basically the parameterisation of the curve.
25:59
And what's interesting with the steering loss is that it actually follows from Fitts law mathematically.
26:07
Actually, not empirically, but mathematically. If you have Fitts law and you have this goal which you try to hit right,
26:12
and you basically do this fourth experiment where you basically introduce more and more goals, and at some point you have an infinite set of goals.
26:18
Actually, if you just do the maths directly and take the limit, you would actually get the student.
26:26
So it follows from Fitts law, which is kind of interesting. Another thing you can do is what's known as simple reactions.
26:32
So basically if something appears, somebody's react to it.
26:40
Simple reaction time can be predicted as well with a functional TD plus TCR, where is the decision time of the human?
26:42
Should I react or not? And tr tr is everything else the non-decision time?
26:51
We can clarify. Everything else in one position come a little bit better.
26:57
We have X, which is the time duration for the perceptual encoding of the stimulus, which we call diffusion.
27:01
The time it takes the brain to make sense of what is shown in front of us,
27:07
and why we set the time duration for the mode of response to activation response.
27:11
So if you show if you show things to people, you can very accurately predict how quickly people can react to it.
27:16
Then you can generalise this into what's called a choice reaction. So in a choice reaction, we're not just reacting to something.
27:23
We have a bunch of options and we can react to these options. So you have n different choices.
27:29
And choice. Reaction is an open loop tossed. You imagine that you have, for instance, free choices,
27:35
and you have three fingers of three different keys, and you just push the key as fast as you can.
27:41
Okay. Let's deconstruct. And a time it takes the average time it takes to make a choice in this kind of for end choice is an open loop task.
27:45
It's going to be b times h will be some empirically determined parameter, which depends on how are we going to do it,
27:56
how big are the choices and what are the keys for pressing and so on and so forth.
28:02
And H is simply uncertainty right. So the entropy in the decision.
28:06
So based on this irregularity we can predict if we put a number of choices to people how quickly people can react to these choices.
28:11
Again, a very robust regularity which has been replicated many, many times since the 1950s and 1960s.
28:17
And this leads to the famous he came along and he is British and Hyman is American.
28:24
I think they basically came up with it at the same time, roughly. And it's basically models the average choice, reaction time T for n choices,
28:29
as the time T is going to be a plus B log base two again to base is arbitrary n, so A and b are implicated some the parameters.
28:36
And basically B controls how strongly time was influenced by additional choices.
28:46
And if there is uncertainty on whether we should respond or not.
28:51
Basically another choice from the ad n plus one. Why?
28:55
Because uncertainty about responding or not is simply another choice.
28:58
When n equals one, we have what's called a simple reaction.
29:02
When n equals two. We have a two alternative forced choice reaction.
29:05
This is used a lot to optimise mobile games and stuff by the way.
29:11
Actually this is very predictable stuff. Okay, so that leads to cognition.
29:15
So what on earth is cognition? Well, we are fiddling around with an iPad and we're trying to do something.
29:21
What on earth is going on from a cognition point of view? Well, first of all, we have control, right?
29:26
We have to be able to figure out, like, should I press now to edit the photo?
29:31
We have to use memory, right? PowerPoint has the ability to edit images.
29:35
So we have to remember that we have to pay attention. As I said before, searching for icons and guiding the theme of the present.
29:40
We have to do some reasoning. Could I edit the photo in PowerPoint and take a screenshot to store it?
29:46
And in the end, we have to make decisions. We have to engage in decision making.
29:52
I don't have time to learn this kind of narrative.
29:56
There are lots of cognitive capabilities, but only some are really relevant to human computer interaction.
30:03
One is called supervisory control. So this is basically the central processing unit in the brains.
30:07
Right. Adaptively deciding to go is allocating cognitive resources to get things done.
30:13
Then we have this issue of memory. We have to remember things. And then we have attention selectively processing some part of the perceptual field.
30:19
We also have to do reasoning. So we have to apply transformation rules of existing beliefs to come up with new buildings.
30:29
Right. Essentially engaging in some form of logic reasoning. And in the end we have to make decision making.
30:35
And decision making is about trade-offs. We do things because we believe it gives us utility, but it comes at a cost, a risk.
30:41
So a decision is not a decision unless there is both utility and risk.
30:48
So what are some general financial conditions that are relevant? Well, a goal is basically something we want to do.
30:57
And a lot of things we have to do is to basically do planning to formulate goals and then figure out how to reach these goals.
31:04
Right. So goals change how cognition processes information.
31:10
So human memory is really less about storing experience events accurately or vertically, as they call it in psychology.
31:16
Rather, it's about access to memories that are useful for a certain task.
31:24
So when we do a certain task, certain memories with activating are brave and useful for that task.
31:28
So here's an example of inattentional blindness, which I talked about before.
31:38
Cognition is goal oriented, which you can see from inattentional blindness.
31:42
I talk to you about a gorilla in amidst paper with the guy in the gorilla costume nobody could see.
31:45
But as you can see, right, people simply do not look at us, right?
31:50
If you look at eye tracking, this has been replicated. Advertisers know this, by the way.
31:54
It's a very well known effect. People are really, really good at avoiding looking at stuff.
31:57
They don't want to look handsome. And cognition is limited.
32:03
So visual attention is obviously spatially limited. Working memory limited.
32:09
We can only keep a very few amount of mental representations in a working memory at the same time.
32:13
Surprisingly few actually very much.
32:17
Think about the typical working memory capacity for simultaneously active things in a mind is thought to be about 2 to 4 items, right?
32:19
Whatever is the definition of an item. So not a lot is going on actually at any specific moment of time.
32:28
And we have forgetting, right? We can't remember everything. And our capacity for abstract reasoning and planning is limited.
32:35
So how can we actually do anything in the world? It's because we can use tools, right?
32:42
And this is known as external revision. We use the world around us to amplify opposition.
32:46
That's one of the main things that separates us from other animals on Earth.
32:51
And cognition based based on internal models of reality. So everything you see is an illusion, right?
32:58
Turns out the matrix movies, right? Right. What you see is not what you get, right?
33:04
We have a screwed up vision of the world, which is very, very, very screwed up.
33:08
But it somehow works because our model of the world seems internal ecosystem to us.
33:12
If that if it isn't, we have a mental health problem. And one part of this is that we have internal models and we create these internal models.
33:18
Often we can exploit that. Not using metaphors, but something is like something else.
33:27
And these metaphors can be exploited in HCI.
33:31
For instance, the very famous desktop metaphor for a graphical user interface.
33:35
We could drag files into folders, and you can drag and drag a file into a trash can.
33:39
And for other metaphors, or can be exploited as well. And cognition is necessary for learning and adaptation.
33:43
So cognitive, motor and perceptual processes are always adapting, right?
33:51
We're always trying to generate new beliefs, figuring out new ways of doing things, new tactics and fine tuning what you're already doing.
33:55
This is ongoing and unconscious. So one example is, uh, motor control for instance.
34:02
So for instance, if you're performing some sort of touchscreen gestures a lot, after repeated about 15 times,
34:08
it would be ingrained in a motor memory for a process known as motor memory consolidation.
34:14
At that point, you can very quickly perform that touch register directly for motor memory, and you will retain that ability for the rest of the life.
34:19
Motor memory is essentially unlimited.
34:26
So there's a lot of things that happen unconsciously when we adapt to what is happening, and we do that throughout our life.
34:28
So people use systems, and the way people carry on using these systems and carrying out their work keeps changing,
34:36
which is a problem for designers, because it means we have to do work all the time, because people do things in different ways.
34:43
We we constantly have people formulating new plans and adapting to new ways of working with computer systems.
34:49
We'll talk about that much later. And we use external cognition notes, calculators, browsers, AI, whatever, right, to amplify what we're able to do.
34:54
And importantly, it requires energy and effort, right?
35:05
So mental effort. Right. The use of energy we need when controlling and thinking and achieving our goals.
35:09
And mental effort can be decomposed into two different constructs.
35:16
Task. Effort. This is basically the response to increasing computational demands.
35:19
Right. So when we face a novel environment, for instance, we have we tried to learn a new user interface or we go to a lecture.
35:25
We are exposed to new material and there's going to be an increase in task effort.
35:31
Another thing that can happen in terms of mental effort is state effort.
35:36
So this is the energy required to protect performance on physiological fatigue, to maintain the performance, to maintain attention.
35:39
Sitting in a lecture for instance. So effort sounds really negative right.
35:46
Because effort is basically something that limits performance but it actually has a positive function.
35:51
I would say that the feeling of efforts protects us from overconsuming energy, right, in less important activities.
35:56
So it's a load balancing kind of construct, what we have learned through evolution.
36:03
And that's about competition. Now I want to talk about needs and motivations, which is another thing that we all have.
36:09
So we work, consume uneven, lead our romantic lives through internet and computer systems.
36:15
Nowadays, for better or for worse, right? But in addition to technical limitations right over human related limits to wider adoption of computing.
36:20
Why is that?
36:29
Well, what we need or what we feel we need, or what we feel we desire really defines what we want to use, what we adopt, and how we adopt it.
36:31
And things are successfully adopted. So at a very high level, there are two ways you can push engineering innovations to society.
36:40
You can do market pool right. Market pool means there's a gap in the market, and we try to fulfil it with some ingenious invention or new service.
36:49
The other way we can do things is technology push.
36:57
We come up with a completely new way to do things like extended reality, and we try to convince people to do it.
37:00
It is, but it's in the end, it's people and their needs that shape both pull and push factors.
37:07
Right. So there are lots of psychological needs for interactive systems.
37:12
Here are some central psychological needs. Everybody has relatedness the need for social relationships which is intimately human.
37:17
Meaning we need a mean for meaning. We need a need for purpose and direction.
37:25
We need a need for stimulation. Actually write novel sensations on thoughts and new exposures.
37:29
We have a need for competence. Believe it or not, we have. We have a very strong ability to perform well in important activities.
37:35
Probably because the monkeys in the flock, we didn't do the job were kicked out of the flock.
37:42
Right? And we have a need for popularity.
37:45
We have a strong need to be recognised by other people, and we have a need for security and need for protection of self harm, right?
37:48
So this Maslow's hierarchy of needs is total pseudoscience. So forget about that.
37:55
That's not real. Real serious science. The real serious science is about these needs.
37:59
And a real actionable way to do things is, uh, self-determination theory.
38:04
So self-determination theory basically has this ID that people are active organisms.
38:11
And what we do continuous through a life is to pursue self-growth.
38:17
So we want to grow as human beings. We want to master things and we want to seek fulfilment.
38:21
And it turns out this is empirically like what people are actually striving people to do things.
38:28
So we don't have this idea of a deficit, right?
38:33
But we have some sort of hole in our soul that has to be filled with various stuff, right?
38:37
Instead of most of what people call motivational dynamics, but basically course positive development and self growth.
38:41
So people are seen as actively seeking new opportunities to market most,
38:50
or instead of satisfying deficits or feeling, you know, filling a hole in your soul.
38:54
And when it comes to self-determination theory of our three principles, the first is autonomy.
39:00
We want to be seen as autonomous. We can do things on our own, stand on our own two feet.
39:06
The other thing is competence. We want to have this sensation of achieving mastery and controlling, controlling the outcomes of actions.
39:11
And the third is relatedness. We want to have a sense that we are in a reciprocal relationships with other human beings,
39:19
that we are recognised for what we are doing and what we are recognising, what other people are doing.
39:26
And when you have autonomy and competence and relatedness, then you get people to really adopt and use things.
39:31
If you think about it like this kind of games we have nowadays where you can play with your friends online, they kind of have that don't say right.
39:39
For better or for worse, right. You have autonomy.
39:46
You can do things. You have mastery. You get better at playing the game.
39:49
And you have this relatedness, right. You have peer recognition of your achievements in the game.
39:52
Right. Using these three principles is basically the way to affect and get people to adopt different systems.
39:57
So how do you basically do this? Well, we're not going to talk too much about self-determination theory, but basically using these three principles.
40:06
First of all, we think about how can we support autonomy competence and relatedness.
40:15
And then we think about these basic psychological needs. The ones I showed you here.
40:20
Uh, sorry. The slides are really slow. For some reason, the basic psychological needs, which I showed you here and how they then yield motivation.
40:24
Right. A sense of reward. Right.
40:33
What is my reward for pursuing my some basic psychological needs?
40:36
And what is the satisfaction I get from basically pursuing my psychological needs?
40:40
And that gives rise like motivation then gets people to say, oh, I should do something in my life.
40:44
So I need to make choices, right? Because the only way to do something in your life is to make choices.
40:50
So that's those choices leads to actions. And those actions yield outcomes, right?
40:55
Consequences. Now that leaves us an update to our motivation.
41:00
So motivation can go up and down depending on what's happening. And then.
41:03
Well, also it's affecting because you notice the environment error going down where it's an environment for action or for something.
41:09
So you need to have this basic cycle, need to work this at the bottom.
41:15
The environment at the top provide you with this opportunity to grow.
41:18
Achieve autonomy, mastery and a sense of social purpose.
41:22
And that gives rise to these motivations which give rise to these actions.
41:26
So that's the framework for understanding how to actually use proper basic psychological needs to get people to adopt.
41:30
They get excited and do things. So learning environments, for instance, should you use these sort of principles to get people motivated.
41:37
This is known as self-determination theory. And it's been incredibly successful in for instance, behavioural or positive behavioural change.
41:43
All right. There are other things we can do as well. It doesn't always have to be a lot of these on our motivations or our motor control.
41:52
It can also be about other people, right? It can be about what is known as collaboration.
42:00
Right. So lots of things we do, we can only achieve because we collaborate with other people, like IDP for instance.
42:04
Right. Which is supposed to be a collaborative project. It's a good example.
42:10
I might use that as an example. Yeah, I used to teach you.
42:17
So I saw lots of robots but never stopped them. Yeah.
42:20
So if you think about it, it's actually quite difficult to see any kind of activity that doesn't require collaboration in some shape or form.
42:24
Right? It's very difficult to see anything that doesn't require other people eventually.
42:32
Right. So because collaboration is pervasive, right.
42:37
Messaging services, video conferencing, shared calendars, or even nitty-gritty things like preparing food for lunch.
42:41
Right. It's something we take. You know, it's unsurprising there are so many collaborative systems.
42:47
There are, however, lots of challenges in designing, building, deploying and supporting collaborative systems to the point that first articles.
42:53
Not too long ago we're talking about a crisis in collaborative systems.
43:01
Actually, we are failing to support people to work collaboratively.
43:04
So collaboration is not only the act of collaborating.
43:09
And that's one of the things people notice, right? It's not just about supporting people actually collaborating.
43:13
It's about support, activities for collaboration, planning, collaborative work,
43:18
being aware of other collaborators goals and actions, and determining what to do next.
43:22
Right. So these principles, for instance, like awareness,
43:28
awareness of other collaborators on what they want to do and what they can do, it's going to influence collaborative practice.
43:31
So we have to separate collaboration and cooperation because they're actually technically two different things.
43:38
So collaboration by definition is a mutually beneficial relationship between
43:44
two or more parties who work towards common goals by sharing responsibility,
43:49
authority and accountability for achieving results.
43:53
So that's collaboration. Cooperation is a different thing.
43:57
Cooperation means division of labour. Okay. Each person is responsible for some part or some global problem solving activity.
44:01
Right. In cooperation with division can be imposed to a particular participant.
44:09
So it's not mutually beneficial, for instance, and there may not be a need to negotiate to establish division of labour during the activity.
44:14
Okay, so collaboration emphasises joint construction of goals, understandings and division of labour.
44:22
So when you did ITP and you were six people you were hopefully engaged in collaboration.
44:28
But all six of you jointly came up with your goals, right?
44:32
I wasn't a team leader who basically told everybody what to do.
44:36
Hopefully if they did, that was cooperation, right?
44:39
Maybe some cooperation. And in terms of collaborative support technology versus this extremely simple, overused model.
44:43
But I will show with you, I don't think it's super important, but it is very things.
44:54
But it's basically trying to think about how can we create what I wanted to call.
44:58
And it basically splits this idea into two axes okay.
45:02
We have synchronous versus asynchronous collaboration and we have remote versus co-located collaboration.
45:06
So an example of a synchronous co-located uh collaboration could for instance be a digital tabletop
45:13
display where everybody sits in front of the digital tabletop and does things in real time,
45:20
is an example of a co-located asynchronous collaboration,
45:24
can be a public display message that you can imagine a message for outside of the quarter here,
45:29
but it's basically a display of everybody can put up a digital post-it note or something like that to hide a message.
45:34
An example of a remote collaboration, but synchronous video conferencing like zoom.
45:41
An example of a remote asynchronous collaboration is sending emails.
45:45
There's a lot of factors that affect the design of collaborative technology.
45:54
One of the most important ones is actually scale.
45:59
So the number of participants involved is really critical because the larger the group, the more coordination you need.
46:02
Division of labour. There is also what is known as communities of practice.
46:08
So communities of practice is a very important term.
46:13
It basically means that a set of people have some related interests together when they form a community.
46:16
So you are actually a community of practice because you are for a few students and you have some shared interests,
46:23
and you can talk to each other about this shared interest in a variety of ways and, you know, cam cribs and all of that stuff.
46:29
Right. So that's collaborative technology, I suppose. Right.
46:34
So you are a community practice. And people form these communities of practice that can be informal for.
46:38
Another, another point of collaborative technology is naissance.
46:45
So basically how much the coordination actions are under development by the participants.
46:49
So basically you can have a very rigid system, right?
46:55
But doesn't allow people to change coordination activities.
46:58
Or you can have a very flexible system that allows people to choose how to collaborate dynamically and explore new ways of collaboration.
47:01
And then you have, uh, you have to think about the factor of planned permanence.
47:09
So the stability of is collaborative engagement. Is it just temporary or will it go away very soon.
47:14
And you have turnover to stability of the groups of participants.
47:20
How many people arrive at this collaborative enterprise and how many people need?
47:23
And when it comes to group interactions, we have to think about tasks.
47:28
So groups perform together to complete something. So we need objectives constraints on other properties of the task to make them well-defined.
47:32
So we know what to do, just like in IDP. We have the environment.
47:39
Action takes place in some sort of environmental context of what are the resources, what are the constraints available?
47:43
We have individual attributes. People have different personality traits, different beliefs, different values,
47:49
and this affects group interaction and behavioural patterns and group structure.
47:56
So in IDP, if you have two people who want to play leader, they're going to start fighting in my experience.
48:00
And we get a very strange planning sequence on the robot form starting out.
48:04
You have group structure and relations.
48:09
So what are the factors for interrelationships among the group members, for instance, affection and also power.
48:12
So the sense of power relationships very important in corporate structures.
48:17
As you as soon see when you get a job. And behavioural patterns.
48:21
Right. These are basically different constructs. What shape the expected or routine way of how us interacting.
48:25
So our assumed roles and divisions of labour within a group, right.
48:32
So we assume people to do certain things. And here's one example of collaborative technology research.
48:36
It's about territoriality when collaborating using what's known as tabletop displays.
48:44
These were very important topic in the early 2000 until about 2014.
48:48
You have a big tabletop display. It's an interactive surface. Everybody can use it.
48:53
And how would people collaborate?
48:57
And basically what we found, we did a study and we basically were interested in what kind of territories of people are forming.
48:59
And they found that basically a tabletop activity can be analysed by zone.
49:05
You have a personal territory. This is a reserved space for your stuff.
49:10
Nobody's allowed to touch my stuff. You have group territory, which is collaborative territory everybody can interact with in the collaborative space.
49:14
And you have what's known as storage territory.
49:22
So this is basically where you store all the things that you're not in use, but are needed for the task, eventually a shared storage area.
49:25
And there are different ways of coming up with these spaces. You can have this kind of directional zone strategy here where you can have this radio.
49:32
So in strategy here. So this is an example of how people have looked into collaboration for asynchronous, co-located action.
49:40
And then when it comes to coordination, there are also extremely important constructs which are actually good life skills to be aware of in general.
49:49
One of the most important things in coordination is to basically be clear about what on earth are we actually supposed to do,
49:56
and how are we supposed to do this? Right now, that's known as articulation work, right?
50:02
It describes activities that are extraneous to the work itself.
50:08
How do we get the work done and how do we decide, right, which talks you supposed to do?
50:11
And how are we going to carry out how we're going to divide things? Who do we need to align with whom?
50:16
And so on and so forth. Very, very important articulation work is a huge amount of overhead for collaborative activity.
50:21
If you do the activity, you don't have much articulation work.
50:29
You just decide what to do and do it in a collaborative setting with lots of people.
50:32
You need to engage in articulation work to figure out what are the rules, what you're going to do, who is going to do what.
50:37
How do we know when things are completed? What happen? What do we do when things go wrong?
50:43
And so on and so forth.
50:47
Another extremely important aspect of coordination is what's called awareness, basically indicating to other members of a team what we are doing.
50:49
Right. We have to up awareness across the team.
50:59
So when these two men collaboration is distributed in space and time, maintaining such awareness is actually very.
51:03
In other words, the left hand doesn't know what the right hand is doing.
51:10
This is a common failure in lots of organisations and in lots of design teams, actually,
51:13
and there's been a lot of work on creating new tools and systems to support collaborative activities to raise awareness.
51:18
Another very important construct for coordination is what's known as a boundary object,
51:27
which is a strange term, but I think it can explain it relatively easy.
51:32
Imagine if I could answer a question here and one of you is up here helping me with this equation.
51:36
Then this blackboard would be the boundary object. Is an object, but basically becomes the collaborative activity.
51:42
So one is an example of boundaryless.
51:50
It can also be things like, for instance, a Microsoft teams shop or some social media app or group shot you have.
51:53
Then that group shot would be a boundary would be from you are sort of collaborating together.
52:00
Right. And the design of his boundary object is actually very important because the design helps people coordinate, share information and so on.
52:05
So you have to think about three critical things when it comes to coordination articulation work.
52:13
How are we going to coordinate. Number two awareness.
52:18
How do we share. And sure typically for technology solutions.
52:21
But everybody's aware about what they need to be aware of in this collaboration.
52:25
And third what are the boundary of which we can design and create to help people
52:29
being aware of what is happening and help people share information and coordinate.
52:34
And that is the lecture for today. There is a lot of further reading.
52:39
So this is basically part two in this book. You don't have to read a book as three otherwise, but if there is anything you want to know.
52:42
This lecture is based on chapters two through nine, which will be used to have open formation among others.
52:49
But all you really need to know is on the slide. So many example papers, but if you want to find out more, you can read about any questions.

Lecture3
So in the previous lecture we talked about understanding humans in general.
7:44
Right. So that's useful. But what is also useful and often necessary in practice is that when we do a design for a particular context,
7:48
a particular interaction context with particular people in a particular organisation,
7:57
we need to know the specifics of user interaction and to know the specifics.
8:02
We need to do some research because this research, nobody can tell us because it is specific to a particular situation.
8:06
And how to do that is known as user research. So user research basically means that we try to find out as much as possible
8:13
about the specifics of a design situation in order to gain design knowhow,
8:23
principles, requirements and understanding of the stakeholders.
8:29
Process whatever is necessary to ensure that we have a successful product system or services.
8:32
So most of these products and systems and services but fail failed because they don't really have a proper understanding of users needs,
8:39
wants, motivations, and so on. So here's the mantra I said before.
8:46
So we said something we said in the first lecture. You are not the user, right?
8:54
Usually we are not the user, right? So we do not know best, right?
9:00
So this is also why we have to do user research. Because we do not work in a specific organisation that carries out specific tasks.
9:04
And we may do things in what seems like a peculiar way. But there are typically reasons why they do it in a peculiar way.
9:11
Right. So basing design decisions based on opinions is very risky and tends to lead to fail, right?
9:17
And sometimes it's also unethical. It cause causes systems that are set to cause harm or distress, which is obviously unethical.
9:25
And basically what we want is basically data that we create just for the research.
9:33
So user research should be about actual users, right?
9:39
Actual users engaged within, uh, within the systems that are, uh, touched by the design.
9:42
Another thing we want to do typically is to also reach non-users.
9:51
So these are known as stakeholders versus a famous paper that identified about 50 different stakeholder categories.
9:55
So there's a lot of stakeholders in play. And for these stakeholders we want to actually understand how they are affected.
10:01
Okay. So what is the aim of user research. Right. So we have theories on principles and they tell us something about people in general.
10:09
Right. And that's useful feed slow and so on.
10:17
But Fitts law won't tell you how to go into an accounting firm and change the processes to make the systems more efficient.
10:19
Right. So there would be lots of questions for the designers that can't be answered by theory.
10:27
And because we can't be answered by theory, we have to go in and do some research, right?
10:34
So we try to gain that additional understanding, right.
10:38
Okay. So the aim is to basically achieve concrete.
10:44
So what we're going to aim for what people call realistic right.
10:49
We're going to obtain concrete empirical knowledge about users for a particular situation.
10:52
So usually we are in direct contact with people. But there are other ways to also uh to retrieve this information, which we will also talk to.
10:58
And basically the tenet here is that we are human centric.
11:07
So we basically try to understand first undefined later.
11:11
And when we talk about design, you will see that this point is actually very subtle and quite important.
11:15
So understand first broaden our horizon and then we design converse to a particular design.
11:21
We'll talk about that more in a later lecture. Well done.
11:27
Okay. Now, the problem with user research is that while it is typically necessary, it is also very, very dangerous to engage.
11:32
Not dangerous to your health kind of thing. But it's very easy to basically obtain misleading or even incorrect information.
11:41
But basically bias is to science in ways that are not helpful.
11:49
One of the problems which I mentioned in the first lecture is to say do problem right.
11:53
People say something, but they do something else.
11:57
So if you ask people how much they use the internet, most people overfilling, most people will underestimate their consumption.
11:59
Same if you ask people how much alcohol they drink, etc., right?
12:06
And there are also a lot of factors that driver behaviour that are called latent factors, right.
12:11
So one important thing is tacit knowledge.
12:16
So basically knowledge about is silent knowledge like is very difficult for people to describe in words like how to ride a bicycle.
12:18
Right. We can all do it. But it's quite difficult to explain how we do it.
12:24
Right. And you basically.
12:29
And there's a lot of this kind of information that people find difficult to articulate.
12:33
It doesn't have to be about motor skills. It can also be things they do as part of their daily job, but they think it's so obvious,
12:36
it's so innate that they don't really think about it anymore.
12:42
So to say to you, problem is one problem we have to face, and the other is it's complicated to get to get knowledge about useless, right?
12:45
Because a lot of user needs, what people really need will be about the future.
12:54
And people are terrible at predicting the future. In fact, people are terrible at explaining the past, right?
12:59
People will be heavily biased, they will forget things very important and so on.
13:05
That's a whole field of psychology called weakness psychology, which is all about how people are terrible at recalling events from the past.
13:09
Right? So people can't remember what happened in the past, and people can't predict what we're going to do in the future.
13:15
So the best way to do user research, just ask people, what are you doing right now?
13:21
Right now, that can be difficult to do. And if you ask people what they want in the future, people say things like flying cars and things like that.
13:25
They don't know, right? They don't know what they need, right. So that's one problem.
13:33
And then there's lots of social reasons that make it difficult. So the way people do work is often people like to keep it secret.
13:37
They don't really want to tell you how they actually go about their work.
13:44
Right? They want to protect their work. Right. And making work visible can make things very difficult for them.
13:47
Right? Who are you really working for? Not the individual you're talking to, but presumably their boss, right?
13:53
So people may not want to relay information to you in an accurate way.
13:59
Now, usually lots of power, place and hierarchical considerations in the workplace.
14:02
So people don't want to tell you the truth. The other problem is that large technical or sociotechnical systems are ingrained in large organisations.
14:07
Use by many people have a huge plethora of different users,
14:16
and it's very difficult to find all of these diverse users, right, and be able to talk to them.
14:20
And if you don't, you introduce bias. And bias is the enemy of user research, right?
14:26
Particularly undetected bias, but sort of steers the research in one particular direction.
14:32
So that's a famous example of Ericsson mobile phones. I don't think they make mobile phones terrible.
14:38
In any case. Right. In the early 2000s you could actually build an antenna integrated into the phone.
14:42
You didn't need an external antenna.
14:47
Yet we pushed out a bunch of models with an external antenna for no reason, and that was based on user research in the mid 1990s.
14:49
But people who had a mobile phone wanted an external antenna for signal and wealthy enough to have a mobile phone.
14:57
Look at my mobile phone with my big antenna. Right. So user research may be accurate at a certain point in time, but then it goes out of date, right?
15:03
And it sort of poisons the design process. It gets us to think in ways that we shouldn't.
15:10
Right? We think about particular ways of doing things, and we ignore other ways of doing things.
15:15
So that is known as bias. And it's an enemy of design. So what is the goal of doing user research?
15:20
Well, first of all we want to find some insights about people, the specifics of people, not the general insights of people.
15:25
So we want to find the insights about people's particular skills, personalities, socio economic status, technical training and so on and so forth.
15:31
But it's pertinent to a particular problem. The second thing is we want to have insights about activities.
15:40
What are the tasks people are trying to do.
15:46
And then we usually want to understand how we're going to use the systems, the context of use, which can be a physical context.
15:48
For instance, the built environment, the liking and so on. It can be a social context.
15:55
So how people basically work together or relate to other people.
16:00
It can be an organisational context which includes power structures and how people choose to divide labour in an organisation.
16:03
It can also be based on a historical context, what people used to do before or what are the established practices and the cultural context?
16:10
What are the beliefs and norms that affect the use of a system? Another thing we also wanna know, and I put it last for reason, is technologies.
16:17
Right? Basic existing systems that people use because people don't want to change and they don't want to learn anything new.
16:25
So if they're used to doing things even in a very arcane way, that might be very high friction for actually doing something else.
16:33
We'll talk about this when we talk about appropriation later. So who is the actual user in user research, right.
16:39
If it's not you, then who is it?
16:45
Well, to find that out, you have to basically explain what is known as the target audience, the people who are affected by the system.
16:47
So since various are famous people with 50 different stakeholder categories, there can be a lot of different people.
16:55
And it's your job to find out. So you have to write out the profile, the group that uses the product, system or service in some way.
17:00
The second thing is having actually written down the target audience.
17:09
Now we need to find people from that target audience. So we need to sample because we can't interview all of them.
17:12
So we're going to sample all of them to understand who they are. Third.
17:18
When we sample from the target audience. Most of the people we really want to talk to.
17:24
But there will be a lot of stakeholders, right? So for instance, if you make an app for a show, the parents are typically going to be stakeholders.
17:27
If you make an app for a severely disabled person, that would be a caretaker who is going to be a stakeholder as well.
17:34
Usually there's lots of different stakeholders that are indirectly affected, and it may be useful to find them as well.
17:40
So how do you do this? And there's tons of Memphis to doing it. One is to talk to people.
17:47
That's called interviewing. Another way is to talk to people while they're doing things.
17:52
That is known as contextual inquiry. I'll talk about this a little bit more in detail.
17:56
Another one is to observe what people are doing. So we just run around and we look at what we're doing.
18:01
And we can also use ethnography. Basically, we try to explore how users are viewing their own interactions that we do with by triangulation.
18:06
For instance by interviewing an observation and so on. We can also do surveys.
18:13
I think you're all familiar with surveys, and we can use what's called diaries.
18:17
So this is not like a diary.
18:21
You're right at home in your room, but basically a tiny little statement you may do every day or every time you do an important task.
18:22
But asked about the things you need to do in order to get things done.
18:29
We can also use non-intrusive ways. For instance,
18:33
we can use log file analysis so we can go through logs and understand what people are using and what are the biggest barriers and blockers and so on.
18:35
And we can also sometimes analyse archival data.
18:42
Right. So things like for instance reviews and so on very publicly available a forum post.
18:46
So these are just some examples of things we can do in general, because it's complicated to get to grips to this,
18:53
and because it's very dangerous to have a lot of undetected bias. We need some form of strategy to go about this in the right way.
18:59
So we're going to have to formulate what people call a research strategy, right?
19:05
Basically, we have to figure out which research methods we're going to use and for what purpose
19:09
and what are free PIM principles that guide people in forming a research strategy.
19:14
So the Prince principle one is that research methods abound in how much they can tell us, okay.
19:19
So they can only tell us so much, right? There is no correct method, right?
19:24
Incorrect methods. Unfortunately, there's a lot of incorrect methods, but there is no correct method, but just methods that are not as bad.
19:29
So we need to carefully figure out right given to go to the user research.
19:36
What are the least bad methods we can use? Principle two is to trade off criteria, right.
19:39
Because again there is no optimal method. So there's going to be trade offs.
19:45
Right. So here are the three criteria. People typically think what is realistic right.
19:48
How real are observations. How much do they model what is actually happening.
19:53
For instance, if I want to know how KPMG does that accountancy software, if I talk to their consultants, I have high realism.
19:57
If I ask you, I have low realism because you have no ID, right?
20:04
Another thing is precision, right? What is the accuracy and the detail of what we actually learn.
20:09
Right. Surveys for accuracy. For precision.
20:14
Right. Contextual inquiry. Observing an accountant standing behind them.
20:17
And keep asking them questions about why we are doing things. Very, very high precision, but also very annoying.
20:22
And then finally generalisability. Right. How well do the findings generalise to other people who are touched by the software design?
20:28
So it's one thing to interview three people and I know learn a lot about those.
20:35
The risk is you learn a lot about those three people, but not the other 100,000 people, but also going to use the software, right?
20:39
So there's this trade off between realism precision and generalisability.
20:46
So that leads to principle three triangulation.
20:52
Basically, we usually want to combine multiple methods to enhance validity and increase Generalisable Generalisability.
20:55
So people know this for instance, want you be able to do political polling. They use a lot of surveys.
21:03
They often follow up the service with focus groups. Right. So that's the principle of triangulation to sort of verify survey findings.
21:08
And then we have to think about the quality. So if one thing we obviously have to think about is the validity of these findings.
21:17
But when you talk to people, are they actually valid. Right. So we have to think about validity from a variety of ways.
21:23
Internal validity. So that's basically a variable that is under the control of the researcher has an effect on observations right.
21:29
And then we have constructed you, which is very important. What we are trying to find out is what actually are we capturing it in a sensible way.
21:36
So I'm going to give you one example which is very related to today's world.
21:44
And that is trust, right? Lots of people want to use AI in organisations.
21:48
And a big thing is due to workers actually trust AI to do that work, right, or do part of their work.
21:52
And it turns out trust is a very problematic construct because you can't really ask people, do you trust the system?
21:59
And they would be like, I don't know, right? Trust is something you develop over a very long time period.
22:05
And also trust can be eradicated very, very quickly, right.
22:10
If a system does something that you wouldn't expect. So you have to think about the contractor himself or the actually relevant themself.
22:13
Sometimes if you have a lot of data, a particular service, you have to think about statistical conclusion,
22:21
validity, all the all findings, just noise or obvious statistically reliable.
22:25
And finally, most important of all external validity.
22:31
Do these conclusions actually hold for any more than the people you just talked to?
22:35
Do they hold for a larger group of people? The total target audience we're talking about are not just the people you happen to engage me.
22:39
And then we have to talk about reliability. So this is basically concerns that the results are consistent right.
22:47
So we want consistent results right.
22:54
So if you use the same methods and use the same measures we want consistent results which is often very difficult to do in practice.
22:56
Another thing we have to worry about is transparency. So we want to make the findings accessible and respectable.
23:04
And this is not just because we want to be good citizens.
23:10
It's because showing data to other people, including people you have engaged with, gives the opportunity for people to correct misunderstandings.
23:12
And finally, we have to consider ethics, right? Whether it's right or wrong in collecting, analysing or reporting data.
23:20
So after all of this, you may ask the question, does which actually work?
23:28
Or is this just a waste of time? So there are a couple of arguments here.
23:32
One is basically about use of resources.
23:37
Some actually are not doing use of resources unethical because you're just guessing and you're usually causing harm and distress.
23:39
There is a lot of research that shows that use of resource can also bring economic benefits, and there are lots of success stories.
23:45
Usually it tends to work, but it has to be done in the right way, done in the wrong way.
23:50
It actually causes more harm than good. And you honestly, you would be better off without it.
23:55
So some things that happen is incoherent. Systems ask SAP basically lots of different random systems because clients suggest a variety of things.
24:00
Feature creep you're basically use like you are hurting a weak signal.
24:10
The use of assertion includes features that somebody said which might be useful in the future.
24:15
And also, we have to remember that users adapt to even the most terrible systems, right?
24:20
So even though we don't get everything right, people will adapt right and figure out how to do things.
24:26
So really the value of user resource is often in actually teaching to designers.
24:31
That is, you and other people like what is actually good design rather than radically innovating new designs.
24:37
There are ways of doing that. And we'll talk about that when we talk about design itself.
24:43
But user research is not a place for radical innovation, typically.
24:47
So how do we do this? So let's go through it very quickly here. We have interviews.
24:52
We can talk to people. However just talking to people is obvious.
24:56
Not research, but just called chatting, right? So from a research point of view, we want to find out important reliable information.
24:59
Right. So everyday conversations are not planned. And that basically means it's a scattershot of different topics.
25:08
We often bias each other by talking to people in a particular way.
25:14
And we rarely record our conversations. Right. So we don't actually have an accurate record.
25:19
And people are terrible at remembering things, as I said before,
25:23
which means put all you're having if you just talk to people is just the noise, basically.
25:26
You need a systematic way. Right. So interviewing here means to learn or use a view of our own tasks.
25:30
So there are a couple of ways of doing it. One is called a structured interview.
25:37
A structured interview can either be quantitative basically based on statistical survey data.
25:41
I'm going to leave that out because it's pretty obvious how to do this. Literally. Read out the survey.
25:46
Another one is to do a qualitative structured interview. That means we have a set schedule of questions.
25:50
Everybody's asked exactly the same questions and we make notes okay.
25:55
We can also do an unstructured interview. There is no fixed schedule or sequence of questions.
25:59
Instead, we basically free right on based on what people are saying.
26:04
We ask new questions. Very, very difficult to do unless you are highly trained.
26:08
This is essentially what journalists do. And it's a very, very, very high skill to be able to do this in practice.
26:12
Then we have a combination of a structured and unstructured interview, which is very useful in practice and that is called semi-structured interview.
26:19
We don't have a rigorous schedule of fixed sequence of questions, but we have IDs.
26:26
We have themes or simple questions we want to ask, but based on responses, we may actually improvise.
26:30
Survey questions. So open ended interviews are the same as semi-structured interviews,
26:36
and this basically means we don't fully plan all the questions, but we have an idea about what we want to find out.
26:43
So we pursue unanticipated but interesting answers and adapt the questions to help the interviewee basically express everything that is interesting.
26:49
We also need to minimise the influence of the person asking questions, which is very difficult.
26:58
And this requires training because if you don't, we bias people.
27:04
Wouldn't it be great if we had feature X? People would say yes, that would be great because they want to please you.
27:07
Humans want to police other people naturally, so we shouldn't ask questions like that.
27:13
So basically the flexible in content and structure. So we have a conversation we can develop.
27:20
But we have a certain continuity right.
27:25
Both conversation partners know where things are going and the interviews about understanding what a conversational partner says.
27:26
So we need to clarify if we don't fully understand what's going on.
27:33
And the interview needs the full attention of the interviewer and ideally, the interviewee.
27:37
Right. So we want to minimise, uh, other things that take up our attention, like note taking and so on.
27:41
This is why journalists, when they interview you, they would have a recording device.
27:46
Right? And of course, we have to respect each other and treat our opinions in confidence.
27:51
Now there is a variant of, uh, semi-structured interview, which is extremely useful in user research, both for contextual inquiry.
27:57
So let's zoom into a practical way of doing this, which is a very fancy name, but it's a very simple idea.
28:05
Basically, we ask people to get on with their work, do their task,
28:11
and while we're doing their work and doing their task, we keep asking them about it.
28:14
Why do you do this? Why didn't you do that? And so on and so forth.
28:18
So we basically do an inquiry in context of use, and that works well because people are really good at talking about what we are doing right now.
28:22
Right. But not what they did in the past or not about speculate in the future, but right now.
28:31
But increases realism, right? Which is very, very useful.
28:35
So how do you do that? Well, again, there is a set of principles to guide us.
28:41
The first important one is context, right? We should be close to the activity.
28:45
So if this is about manufacturing, we should be in the actual factory talking to people next to the machine.
28:49
Right. Not somewhere else. It's also a partnership, right?
28:55
Basically, it's almost like you have like some sort of master apprenticeship relationship where the
28:59
interviewee is the master and the interviewer is the apprentice who wants to understand,
29:03
how do I actually do this? Right. What is going? A third thing.
29:08
What is important is interpretation. So basically both conversation partners have to help ascribing meaning to the actions.
29:12
It's very important for the interviewer to clarify things to ensure that they fully understand what is going on.
29:20
And finally, focus. We should go for depth.
29:26
We should fully try to understand what is actually going on. So that's called contextual inquiry.
29:30
And as in all interview techniques, people need to be trained to do this and it takes experience to do as well.
29:35
When you analyse it, obviously we will engage in transcription.
29:43
Then we can take it. Then we can do some analysis. People typically do themes where you have thematic analysis of themes that pop up.
29:46
It may, for instance, do some task analysis and figure out which talks lead to what?
29:53
Creating flowcharts and whatever. And once we have done that, we have to engage in verification.
29:58
So return to the interviewees and confirm or understand they are correct.
30:03
And finally report the outcomes so the design team can actually action the findings.
30:07
Now again we get to say do problem. And a big problem with interviews is to say do problem right.
30:14
So the key question here. Is it possible to obtain accurate information about users practices by asking them.
30:21
Well is it sometimes. Right.
30:28
So there's a lot of discrepancies of what people say and what they do.
30:32
So one interesting thing is handwashing, as you can see here.
30:35
If you ask people if people wash their hands after going to the toilet, pretty much everybody will say yes.
30:38
If you actually use CCTV evidence, right, of public bathrooms and so on.
30:44
You can actually see if people wash their hands or not.
30:49
As you can see, there is a significant proportion of men and women who in fact do not wash their hands after going to the toilet.
30:51
Right. So people say one thing, but then do something else.
30:56
And they do this for a variety of reasons. Inability to imagine the future.
30:59
They cannot actually express the knowledge or social reasons.
31:04
Like like hand washing. Right. You want to come across as a good person.
31:08
So this is a fundamental problem with interviewing right? They say do problem.
31:11
We can also do what's known as field research. So this is basically a room where people are actually supervising a command.
31:17
Command and control room for running the a competition in northern Finland or something like that.
31:25
And you can see a lot of people and lots of stuff is going on, lots of projectors and so on.
31:30
And what you do in field research is you go out in the field and you observe what people are doing, right.
31:35
So you want to minimise the bias of the researcher being present. The researcher ideally should be almost invisible, right?
31:42
So there's a lot of things that you can only find out from field research.
31:48
Typically context reasons, the set up of the room itself, the social aspects,
31:52
the relationship between the workforce, which is very difficult for people to express.
31:57
Right. Very difficult to express, even if it wasn't helplines and the technical solutions like how people use digital artefacts,
32:01
people often do things without explicitly thinking about it. And the method we're using is observation, right?
32:08
And we want to observe a bunch of things,
32:15
and we want to basically collect notes about our observations to account for frequencies and importantly, patterns.
32:18
Right. So we want to position ourselves so we don't interfere with the process.
32:24
And this is sometimes called the ethic view ethic or ethic.
32:29
Now those things we have to consider here. For instance, what is the site of observations?
32:35
Where do we where where do we observe the users. And we have to think about that.
32:39
We don't want to affect what think what is happening. This is not reactivity.
32:43
You can do shadows of following people and you can.
32:48
And we have to think about the data capture and note taking for verification.
32:51
So we have field notes field diaries unstructured note.
32:55
So we have different levels of notetaking going on. Now the big problem with field resources.
32:59
But if you go into a factory and if you haven't I would recommend you deal with this kind of mind opening.
33:05
There will be lots of people running around doing things. And obvious question if you're going to do field resources.
33:10
What should I look for? Right. What to look for. And that's the fundamental problem of field research, right?
33:17
In fact, all user research, but primarily field research. What should I look for?
33:22
Right. What is going on? There's so much going on. How do I structure my thinking?
33:26
So I understand what's going on. So this is a common problem. And because for this reason people have come up with frameworks.
33:30
So you see one example of a framework here.
33:36
To the right we can think about spaces the spaces people office, the spaces, the actors, which people are going on,
33:38
where the activity objects people are using, the actors, what people say and what they do, events, things that trigger people to do something.
33:46
Time sequencing shows what people are trying to accomplish and things.
33:55
And this is just one example of a framework, right?
34:01
And these frameworks are very important for theorists because if you just go in somewhere, you will not understand anything.
34:04
If you just observe it, you won't make sense of it.
34:09
You need to come with some prior knowledge or some prior structure to basically attach your findings to.
34:11
Otherwise, you're just going to record random stuff and that doesn't help.
34:17
So here are some principles for analysing these field notes. One is immediate recall right.
34:23
So you basically want to write down things right away. Another one is what's called thick description right.
34:28
So there will be a lot of events that are unimportant. And if you just recall everything,
34:34
you just end up in a massive amount of information that nobody's going to use whenever something interesting is happening.
34:37
You have to scale, you have to have the skill to detect that and then go into much more detail.
34:43
That's known as a thick description. You also want to do coding.
34:47
So you want to. Develop a coding scheme that is reliable and reproducible.
34:52
So code should be unambiguous. And you want to have coding definitions with examples and counterexamples.
34:56
Everybody understands it and write it down in the coding ninety-firsts.
35:02
Most of you know systems that can help with coding and analysis nowadays actually.
35:05
And you also want to do validation with participants, right?
35:10
Because the conclusions that you draw based on field research are basically statements about the people in the particular context we serve.
35:13
So they are stakeholders, right? So they they you should give them an opportunity to revise and look at your conclusions and revise to clarify them.
35:20
So the big problem here is can field research inform design.
35:32
So the strength is realism because we go into an actual situation and we observe what is happening.
35:36
Right. That gives us very high realism. We have a defect to account of what is happening, you know, ICU and so on.
35:41
But there's also a lot of problems, right? One of the field research is very costly.
35:48
We have to pay people to run around and observe people all day. Right.
35:52
And also meat realism is also a problem, right. Because if I go into one of ICU and other groups and I make a very, very detailed description,
35:56
chances are that that data description will only be applicable there.
36:04
I know that otherwise. Right. So going for very high realism reduces generalisability.
36:07
So there is a trade off. On the other hand, there's lots of non-obvious problems that affecting computer use.
36:13
You can only find by field research, right? Usually it's very good at detecting new kind of practices.
36:19
What people are doing with new technology. Now, sometimes field observations lead to radical new design insights.
36:25
And also field research has what's called a narrative power.
36:33
It means that it's possible to write very engaging descriptions about what people are doing,
36:36
but makes everybody into the same team and affirm them, empathise and get them energised and understand what is going on.
36:41
And that can be useful in its own. Okay.
36:48
Another thing you're very familiar with is survey. So do you. The department here does service all the time.
36:53
So service basically means we come up with a questionnaire. We ask people to ask for questions.
36:58
There are a couple of things we can look for. We can look for behaviours, activities or routines.
37:03
For instance we can ask people about experiences. How do they experience a particular service for instance.
37:07
We can also find out needs, what they desire, what people want and attitudes, right?
37:13
For instance, people's preferences and people's beliefs.
37:20
The key advantage of survey research is the opposite of feelings, which is generalisability, right?
37:23
We can also be service tend to lots of people, cheap and cheerful. We get tons of data back, right?
37:28
So how do you design server research? Well, first of all, you have to have a focus.
37:36
You have to identify the goal of the research because service is essentially a measuring tool.
37:40
And it's a very, very dangerous measuring tool because whatever you're measuring you're going to measure a lot of it and nothing else.
37:44
So it's very easy to measure the wrong things. Right.
37:50
So you can use service to collect qualitative data at scale to status study.
37:53
For instance people's attitudes and diff and survey types can be split basically which can be either descriptive or analytic.
37:57
So descriptive service we're not going to talk about much less like a census survey.
38:05
We just have to indicate the number of people in your household and so on.
38:09
Another one is the analytics survey.
38:12
It provides some sort of understanding about why a certain situation exists by comparing variables and the stories told by people,
38:14
well, basically responses from different sets of people. And a very important thing when it comes to design of survey research is sampling, right?
38:22
How we approach people and how people respond to a survey.
38:29
For instance, if you submit a survey on a Friday at 5 p.m., I can pretty much guarantee you,
38:33
but only the cranks who would like you a survey, and they would have very strong opinions about whatever they have strong opinions about.
38:39
And you get a massively biased sample. There are also known effects like a coffee effect.
38:46
If you send a survey ten minutes after people had a cup of coffee,
38:51
that would give you better results and then you would have higher engagement, right?
38:54
So surveys the danger is you don't know these people, right? So it would be very, very biased results.
38:58
Right. Depending on when you send in the survey how you write the survey and so on and so forth.
39:03
So surveys are cheap and cheerful, but they can give a lot of undetected bias that can be accidentally infused into the same process.
39:08
It's also very difficult to write a questionnaire actually writing a questionnaire, but it's actually methodologically correct.
39:16
It's very, very difficult and best left to experts. So it's best to use existing questionnaires.
39:23
So I won't read all of them.
39:28
But here are some examples from very highly established validated questionnaires that people use are using existing questionnaires.
39:29
It's way better than inventing your own because chances are high you would get something wrong.
39:37
Customer questionnaires have lower reliability, and lower reliability is bad.
39:44
We also have to process the answers. So we have to shift answers, clean them and prepare for further analysis.
39:50
We have to remove answers that are invalid and so when or so forth sometimes.
39:56
But people are found to be inconsistent, which is one of the things in service,
40:00
to ask the same question in different ways, to see if people are consistent.
40:04
People who are inconsistent may also be removed. Should we remove the participant in time?
40:08
That's called least wise deletion, or should we just remove the problematic answers, which is not pairwise deletion?
40:13
And if a large proportion of the data needs to be removed, it probably wasn't a good survey to begin with.
40:19
And then we can compute reliability. I will keep this very brief.
40:25
You can look at it later. Basically there is something known as integrator reliability how reliable people are.
40:28
And we can use different constructs like Chromebox Alpha Alpha which is a common way to do this,
40:34
to basically figure out how consistent people are in answering their questions.
40:39
Um, people have come up with some magic numbers of Presto for when something is considered reliable or not.
40:43
So here are some examples of people have used this. How do people use Facebook.
40:49
So they had a survey about that and they basically did it over a number of years.
40:53
And we also used that Facebook as part of my everyday activity. And Facebook has become part of the daily routine.
40:57
I am proud to tell people I am Facebook. This must be early days.
41:03
It is in fact early days. Facebook is just a fad.
41:07
I would be sorry Facebook shut down, etc. right?
41:11
So you can also follow these kind of questions and then you can find out how do people communication with other people on Facebook changed over time?
41:13
How has the respondents perception of audience of Facebook changed over time?
41:22
And how has the response attitude towards Facebook itself changed over time?
41:27
So by asking the same survey on a repeat instances, you can basically develop an understanding of trends, right?
41:31
So over the years, Facebook has become increasingly part of daily people's daily life, as suggested by answers to the first two questions.
41:38
Right. Now this is perceived by people to be useful over the cycles appears to be increasingly cause problems for respondents.
41:45
So don't be too much on Facebook or whatever, right? So this is a way to use service.
41:53
You can do repeated service with a large sample. You get a generalisability advantage if you do it on a repeat basis you can look for trends.
41:57
Then finally, there is a way to do user research without actually bothering people at all.
42:08
And that is known as unobtrusive research. So the idea of unobtrusive research is this basically reactivity, right.
42:13
We remove reactivity altogether by simply not bothering people directly at all.
42:21
Right. They do not know we're going to do user research. Right.
42:26
So unobtrusive research is about traces of use of behaviour or archives to make inferences about what people are doing and their activities.
42:29
So for instance, we can analyse application logs. This is very, very common.
42:38
In fact, every big web system will do this. We can examine records of posts and comments in social media.
42:42
Again very very common also for finance purposes. Now we can use, for instance, publicly available YouTube video.
42:48
So that was an interesting paper a couple of years ago,
42:54
but use publicly available YouTube videos showing the incidence of new incidents of self-driving cars and how people reacted to that.
42:57
So there are lots of sources for non-reactive data.
43:07
I think one of the prominent ones is traces log file analysis.
43:10
So this is a very common thing to do. I have a lot to say about log file analysis because it's easier said than done.
43:14
Another thing is direct traces. So these are recordings.
43:21
But of course by user's action right. Mouse movements and clicks that can be logged in operating system.
43:24
We also have indirect traces right. But basically only indirectly by users by some intermediary mechanism.
43:30
Right. So this is something, for instance, you see in a park, right.
43:37
In a park you can see a path, but it's basically created by people ignoring the roadways and basically taking a shortcut.
43:41
Right. That's an indirect trace. Now we can also use archives.
43:47
One of the most common things is log files, where we basically have device events that are low level events.
43:53
We have user interface events that we typically focus on opening windows.
43:58
We have application events. So actually doing some sort of task and load data has of course to clean.
44:02
And you have to do a lot of data wrangling to make this work. There's a lot of attention to how to do this.
44:09
I will tell you what is most important with log files, but is whatever you want to find out.
44:14
You basically have to write that down before you write your logger, right?
44:19
If you just log all the low level information, it's going to be very, very difficult to actually reason about this information lake, right?
44:24
Because you have so much low level information.
44:31
If for all things you really want to know, let's say it is how much people delete particular files, you should load these instances specifically.
44:34
Right. And that would make the log analysis much more useful. So log log analysis really comes down to hypothesis right.
44:42
What are the hypothesis that research questions you want answers to.
44:49
And if you know those. So if you have specific in order to answer those questions.
44:53
And now log file analysis is a lot easier and a lot more reliable.
44:57
So the mistake is you go to a client and we have tons of logs, but it's all low level logs right.
45:02
So there is no context. There is no semantics. It makes it very difficult to figure out what was actually going on.
45:07
So we need to think before ahead of time how we actually log data.
45:12
So here's one example about Fitz law. Does Fitz law actually apply to real world pointing?
45:19
So they basically logged, uh, movement kinematics of 24 people in 36, uh, computer configurations over several months.
45:24
I don't think this was ever published as a technical report from Paris or something in 2007.
45:32
But basically they found a of lot of people moving around with the most they could find to find some fresh source to get rid of things.
45:37
Some people are not actually making any movements.
45:43
And if you got movement time as a function of index of difficulty, the difficulty of hitting a target,
45:46
you can see that you can you get a linear regression line somewhat in that data.
45:51
So that's an example of logging. We can also instrument people which can be much more interesting.
45:57
And this is what we do.
46:03
For instance when we want to find out what is happening in manufacturing environments, policing, environmental air traffic and stuff like that.
46:04
So what can we instrument? We can instrument people.
46:11
So we can basically fit sensors to people to understand more about people, including emotions or stress and so on.
46:14
We can instrument things. So people have done that for a long time.
46:20
Mobile phones have a lot of useful centre sensors.
46:24
For instance, TikTok uses accelerometers to basically infer if you're looking directly at a mobile phone or watching a video or not,
46:26
and use that to optimise flow metres. You can instrument places, so we can put in sensors in the lecture rooms,
46:33
etc. to track that people are paying attention or moving about or whatever, so that we can instrument people, things and places.
46:39
And this can give us a lot of information. Again, if we know what the research questions are ahead of time, right?
46:46
Otherwise we just get a lot of sensor data. But it's very difficult to make sense.
46:52
We always have to think about what do we want to know and then do the instrumentation right.
46:56
What are the research questions we want to have answered?
47:01
But instrumenting people is a great way to get a lot of data, a lot of insight and reduce reactivity.
47:04
Right. People don't know we're doing the research really. They can just go on and do what they always do.
47:10
Which is what we want. We can also go into archives.
47:15
There's a lot of things available. YouTube, social media, forums, etc.
47:19
There's also like bug reports, things we can look into and add all kinds of user generated content.
47:24
And we can learn a lot about that. And people use this as a treasure trove for understanding what people are dealing with interactive systems.
47:29
And our cattle. Data is frequently used in what people call content analysis.
47:37
So we basically wants to attach some subjective interpretation of the content of people have been destroyed.
47:41
So one common example is social media. People write social media posts and people can then analyse the politics of that to a motive.
47:47
So back to emotions engaged and so on.
47:55
And there's a whole research field about understanding what basically causes a post to become viral and what what are what is a collective
48:00
sense of posting on social media and video sites actually telling us about people's sentiment to the stock market and the society as a whole.
48:09
And so when I saw.
48:16
It's also possible to do some deeper analysis, so it's possible to understand the dynamics of conversations and how conversations can go astray,
48:18
where conversations can actually be focussed on how people can achieve consensus, and so on.
48:25
And all this information is already available. No need to collect it again.
48:30
So that can be very, very valuable. Of course having done all of this right.
48:34
So this is just some sample of user research methods. We have a lot of data.
48:40
But now is this data actually useful for it to be useful.
48:44
We have to represent the data into something that is actually actionable for the site.
48:48
So this is called creating representations. So user research right is to inform design decisions right.
48:52
So we need to create a representation that can help us inform design decisions.
48:59
Right. So we have to represent the data, typically using diagrams models or text.
49:03
And how do we represent findings will bias design teams in various ways.
49:10
So the representation matters a little. So here you see a very simple example of free form.
49:14
So let's do so. And here you can basically see a very simple task analysis which is presumably created based on user research.
49:19
And there's a couple of key ways people represent things. One of the most common ways is called a persona.
49:27
So a persona is a statistical aggregate of actual people.
49:33
So we create a fake person state by aggregating statistical information about people in general, clustering people and then giving them a name.
49:37
Right. So the idea is to construct these archetypes of users, which are fictional but represent individuals with particular characteristics.
49:46
So why would you want to do this? Why would you want to create a persona?
49:54
Well, it's because you want to create empathy into the design, right?
50:00
The design team wants to emphasise with people. If you create personas, you can ask things like, what does Camilla think about this design?
50:04
And everybody knows what you mean. So here's an example to the right about a particular persona description.
50:11
Personas are incredibly common in design. So what is the advantage and drawback to advantage if you create specific individuals, right.
50:18
So it makes us keep sick, right? It keeps the design grounded.
50:27
When we make design decisions, we can think about how it will affect a specific set of personas, and that is useful.
50:31
It also avoids this kind of you are not the user kind of thing because you have you're forced to think about other people than yourself, the personas.
50:37
They also help us prioritise design.
50:45
We can basically ask about how does this affect the different personas and help help to do this, for instance prioritisation based on that.
50:47
Uh, we synthesise data. Raw data, as I said, is often too complex, right?
50:55
Too difficult to use under personas or essentially high level clustering, but makes the user data more, uh, actionable.
50:59
And again, as I said before, we create empathy. It's much easier to relate or person's viewpoints, right?
51:06
If it's a person rather than, you know, a summary of your data.
51:13
So the drawback is that personas are often created in a bit of a haphazard way.
51:17
So the validity of the personas can be under question.
51:21
Right. Personas can be almost anything, and the connection to the original data can be broken with.
51:25
This basically yields misleading personas that look very convincing to the design team,
51:30
but are not actually properly grounded in in data insights from user research.
51:35
And also personas can be perceived by design teams to not be believable design by committee,
51:41
right because they are aggregates or lacking like original link to data for the problem because of the problem I just mentioned.
51:46
Other things people are doing is you can create scenarios. For instance, if people are, the accounting firm is going to buy something back.
51:55
So you have to procure something. You have to go through children's systems.
52:02
Procurement systems are a nightmare to deal with because there's lots of regulatory actions.
52:06
We can write scenarios, and a well written scenario helps everybody understand what is going on,
52:10
for instance, in procurement and why is it a problem or what's the scenario and how are people involved.
52:16
Another thing we can do is what's called customer journey.
52:21
So customer journey is essentially, uh, essentially a glorified flowchart but relaxed in terms of diagrammatic notation.
52:24
It basically shows what people have to do in order to get the thing done.
52:31
And typically, customer journeys are designed as a sequence of touchpoints, touchpoints when customers interact with a service.
52:35
Right. So we basically map events that precede actual use to understand what we can do to help people, to engage people into our process.
52:43
We can also analyse tasks. So we basically figure out how people are doing things task one, task two, etc. and how they relate to each other.
52:56
This is basically a functional description of behaviour. What do we have to do?
53:05
So one task follows another. Uh, basically, the output of a task analysis is a task model.
53:09
A sequential task model is literally just a sequence of tasks.
53:15
But we can also have hierarchical task models that can look something like this.
53:19
So we have some sort of order and task can be split into subtasks and so on.
53:24
And operations can be a condition. We talk more about this in the talk about engineering and systems, in particular,
53:28
about the ways we can basically create sophisticated models of how people have to go about their work.
53:34
We can also represent context. So I'm actor can be anything a human or a system.
53:42
And we have different context models that can describe what is going on.
53:47
We have the flow model which is based on flows of energy signals and people.
53:51
And so we have a sequencing model which follows action sequentially in order to achieve a goal.
53:55
We have an artefact model where we describe interactions among artefacts for a workflow.
54:01
For instance, we have a cultural moment where we just got different beliefs,
54:05
and people can use constructs such as Venn diagrams to indicate cultural compatibility.
54:09
We can have a physical model where we render the physical space and actors movement in space,
54:14
like space time cube analysis, for instance, which models how people move around in space as a function of time.
54:19
And sometimes it's just very difficult to make sense of stuff. And simple things can be really useful.
54:27
And a very simple thing to represent something very messy in one picture.
54:32
So everybody knows and is grounded and looking at the same thing is what's called a rich picture.
54:37
So a rich picture is not diagrammatic and that's not set syntax or notation.
54:42
It's just to give you the gist of what is going on. So here's some example of a rich picture of a cup.
54:47
We have the pub itself. You have the landlord bought this one in order to give us an increase in the envelope.
54:52
And then do some type of wondering the same thing. You know, the customer wants value for money.
54:59
Unfortunately. Facilities. You have to worry about other parts, the competition.
55:03
You have to worry about the police, closing time, drunk driving, etc.
55:08
And you have to worry about the community. Antisocial behaviour, noise disturbances and need to keep it.
55:13
That happens. And then of course, the pubs tend to be owned by breweries that they have.
55:18
They make capital investments based on expected profits.
55:22
Right. That all of this complex narrative can be shown quite elegantly and succinctly.
55:26
There is which picture, which just highlights two main factors that are affecting the operation of this pub.
55:31
And of course, we can come up with requirements. We're going to talk about requirements later.
55:38
So I would leave it to their basic requirements. And here we are talking about user centred requirements or requirements elicited by user research.
55:42
Of course requirements have to be unambiguous.
55:50
They have to be testable. And one useful thing when it comes to requirement is a use case.
55:53
So a use case basically enumerates or elaborates on actions that are useful must take to achieve a goal.
55:59
So are you a use case? Typically you need to explain the initial state of preconditions that have to occur,
56:06
the goal and then activities the flow of events that lead to the goal being satisfied.
56:12
We'll talk more about requirements when we talk about engineering a systems later.
56:17
But there is one more thing I want to say about this. I'm going to use a story.
56:21
So this is basically I'm going through some informal express use cases,
56:26
basically explaining how people go about things in order to achieve a goal written from the perspective of the user itself.
56:30
So what can we do with representation? So what can we not do?
56:38
What verifiability is basically testing how well the claim can be cross-checked with observations.
56:41
So one thing that is very important is traceability. Can we trace through code or representation.
56:47
So I can look at the representation of point at any point in a diagram and actually
56:53
understand what piece of user resources yielded that piece of the diagram.
56:57
And that's all you some research ala computery by the statements of other users.
57:01
There are two ways of thinking about this. The first is called instrumentalist view.
57:06
If user research is useful for design, so be. Yes.
57:11
Let's go with the flow. The out of you is the realist.
57:14
User research must be validated. In practice it will be somewhere in between because there is no perfect user research method,
57:18
and it's very difficult to do user research really, really well. So in summary you are not the user.
57:25
We need to find out specific information about use cases.
57:30
We do that for user research. So we want to produce actionable knowledge about users for a specific design situation.
57:34
So all user research methods are limited. With respect to realism, generalisability and position.
57:41
So the key qualities are use of the social networks, concerns, validity, reliability, transparency and ethics.
57:48
And in the next lecture we'll talk about something completely different.
57:55
We're going to talk about the different models of interaction. 

Lecture 4
So we're going to talk about different theories of interaction that allow us to model interaction from a variety of perspectives.
6:11
So first of all, what makes a good theory of interaction.
6:19
Well, first of all, a theory should have explanatory power, right?
6:22
It should basically have empirical accuracy. So when we go out and do testing we actually see the test results we expect based on theory.
6:26
We should also have broad coverage to cover a variety of situations.
6:35
A theory that is very narrow isn't very useful in practice.
6:38
Theory should predict. So they should predict what could happen right with some probability.
6:42
They do this by having models. So Fitts law for instance, is a very simple model of target acquisition.
6:48
But allow us to predict whether an interface is more efficient than another interface.
6:53
And this leads to the third point. Theories help us evaluate systems okay.
6:58
We can use theories to guide us. Sometimes we don't need to involve users at all.
7:03
And if we do, we can use theories to explain to us and help us describe the best ways of finding things out.
7:07
So theories can guide measurements. What kind of things we can measure, what is meaningful to measure and accurately measure.
7:13
And finally, theories tell us what to do. Just like in any sort of engineering science discipline, we can do it by abduction.
7:21
We can explain observations through theories. You observe this result because of some theory we know about.
7:29
You can do things through deduction. So theory can suggest the best way to design things.
7:35
And you can also do it by counterfactual reasoning. So if the design was x, then the interaction might be y.
7:42
So theories are important. And when it comes to theories, let's start with there are a variety of perspectives.
7:50
Some are highly qualitative and some are highly quantitative.
7:57
We start with a quantitative one. Basically theories of interaction based on information theory and control theory.
8:00
Okay. So you already know about self information and you can.
8:06
If you don't know about the binary entropy function, you can read about it on the slide later.
8:11
I'm not going to waste time talking about this kind of stuff. I would say that we have entropy and we have this idea of redundancy,
8:16
which is the difference between the actual average bits used for communication compared
8:23
to the average speech required for communication using an optimal coding scheme,
8:28
which you should remember from one. The somewhere perplexity is simply two to the entropy.
8:32
Technically, is the weighted average number of choices a random variable have have to make the higher the perplexity to hold of the task.
8:38
Mutual information you have known about before is expressed as follows.
8:48
And what does this all have to do with interaction? Well, you can use information theory to do some reasoning about the interaction.
8:52
Right. So one of the things we typically can be interested in is rate right.
8:59
How quickly we can shove bids from one end to another end.
9:03
And when you're talking about interaction,
9:07
it's basically the rate to say how quickly we can shove bids from the user's brain their intention to a computer system,
9:09
but actually can action with intentions. So we can do that by having a rate where we have the mutual information over some time.
9:16
And then you can rearrange this, uh, based on conditional entropy, which we can see here.
9:24
But if the probability of error is zero, I believe there cannot be any error in the direction.
9:30
We simply have we seem to reduce this to the entropy of whatever we tried to communicate over that time period.
9:36
So this can be used to reason about how efficient devices are.
9:44
One of the classic things I've talked to before is fixed norm. Basically, users are basically asked to point as quickly as accurately as possible,
9:49
which has different targets with different index of difficulties, different sizes and different distances.
9:57
And then we basically compute movement time which is linear regression a plus b.
10:03
And what I said is this index of difficulty. And now we can compute through basically how quickly the human brain can communicate
10:07
bits through some pointing device to actually allow us to do target acquisition.
10:15
So if we have throughput we can we do average, we do some difficulty over the average movement time, which we can measure.
10:20
And the downside view is that we rely on average index of difficulty, which makes sense if all buttons are at the same size.
10:26
And we tend to have the same distances. Right. But may not make sense if the buttons are very different than position and very different distances.
10:33
The other way to do it is to look at the slope one over B.
10:41
And of course, the downside here is that the slope is going to give us an indication of rate of course, but we have to ignore the intercept.
10:44
And intercept can be significant for some pointing acquisition tools.
10:52
And people use these kind of definitions of throughput to reason about.
10:57
We have a pointing devices, be it 3D pointing or touchpads or capacitive touch screens, how efficient they actually are.
10:59
And there is an ISO testing standard for this. So yes, as an example, people can do all kinds of measurements about this using throughput.
11:06
So here you can see throughput in beats per second as a function of different devices that allow people to define a tolerance in different ways.
11:15
And you can see you get a lower throughput for the tablet on the laptop,
11:22
and you get a higher one for a public display in the table top and so on and so forth.
11:25
And you can also then look at which muscle groups people use, because people tend to optimise and use and people try to reduce fatigue.
11:30
Right. People try to reduce fatigue by optimisation.
11:37
So they basically use muscle groups in sometimes unexpected ways, which can be used to inform design of these sort of devices.
11:40
Here's a crazy example from this university actually. Well, physics department.
11:47
It's called Dasher on this by the late David McCarthy, who used to teach information theory in the Cavendish lab.
11:51
And he came up with this interface as a teaching tool.
11:57
The basic idea is that you have a cursor here if you position a and when you drag this cursor to these letters, these letters are in boxes.
12:00
And the size of each box is proportional to the probability of you wanted to write that letter as defined by a language model.
12:10
Once you go to that point, for some reason, you can make a T.
12:17
So T is going to have a very high probability in English because t is a very frequent letter.
12:20
Think about the, for instance, 6% of the English word calories. But the T is going to be a huge force when you going to T zooms in dynamically, right?
12:25
And then you see H because THP is of course going to be highly probable other easier medium and so on and so forth.
12:34
So you basically navigate through this dynamic exchanging interface.
12:40
So it's information efficient, but it relies on closed loop interaction, which we may remember from lecture two.
12:47
And what do we know about closed loop interaction is a visually guided interaction.
12:53
And that is going to be slow compared to open interaction because the visual system has to be involved here, right.
12:57
This is essentially a navigation task, right. So it's the most intensive closed loop interaction.
13:02
But it is information efficient. We can actually analyse this thing using some basic information theory.
13:07
If we have some rate d bits, is that right?
13:15
But attempts to zoom into the region. So do you use a rate the right at the rate rate RDF in pitch.
13:19
Then the tend to zoom in to this region. Uh, basically, the users intend to text by a factor of two to r d.
13:26
What's the complexity of two to the two to the rate? And then if you have a language model that generates text, right.
13:34
Because you remember for the books that generated by a language language model,
13:41
and then if it does not have an exchange rate to power left, then you use an able to reach a rate of all over all 11.
13:44
So are these the usage rate or is the exchange rate?
13:51
You can also control Dashlane by discreet button presses so you don't navigate using a mouse or something like that.
13:56
Basically, you press a button button, right?
14:02
And you basically your timing. I'm assuming these bottle prices are precise.
14:06
So with no errors, right? The capacity to be shallow.
14:11
It seems to be that the log base two of the number of uh, functions over time period that you're using,
14:15
if the process is noisy, you're not always hitting the right right button.
14:22
You're going to have to model that noise. And you can do that using the binary entropy function.
14:27
But it's interesting with this is not the analysis per se.
14:31
It is that this analysis gives rise to very counter and counterintuitive results.
14:35
For instance, even if you have a quite a low error rate of about 10%, you could use this analysis to channel capacity.
14:40
How much bits you can shove over from the use of spray. And the system is actually reduced by a factor of two.
14:48
Right. So even a small error rate, right, actually causes a large reduction in channel capacity,
14:54
which I think is very difficult to detect experimentally or intuitively.
15:01
So you can use the analysis, you know, information theory to do this sort of analysis as well.
15:05
Another way to look at things is through the lens of control theory.
15:09
So in control theory we basically view the human computer interaction as interaction as a control system.
15:14
So we have a human built within this dashed rectangle. So as a human we have some sort of control group.
15:20
We want to reach from the side of state, you know, state space which can be discrete or continuous.
15:25
We're going to do an issue. We have control signal through some control effector, let's say a touchpad or a keyboard or whatever it is.
15:30
That means we're going to affect some control variable. That's what that variable is going to be the influence of noise.
15:38
It's also going to be sensor by sensors that give rise to something else on that, which is a feedback control.
15:44
Right. Something happens in the system. So it's a closed loop control system.
15:49
I'm just going to give you one example. There are more examples and exposition in the book.
15:55
But just to give you an idea, we talked about Fitzgerald before,
15:59
which is basically modelling target acquisition based on the lens of information theory.
16:03
The problem with information theory is it doesn't explain any mechanism, right?
16:07
It only gives you a headline result. Some device has a higher throughput than some other device.
16:11
But why? Right. Why? And you can use control theory to dig a little bit deeper.
16:16
So if you have some reference variable you are right.
16:22
So all corresponds to the distance right in the target to the target in the feed slot.
16:25
In other words, D you know that the d that we used in the equation for index of difficulty.
16:30
And if, you know, assume you have a change from the hole position to the target position.
16:36
And that is a step change right in the reference variable, do you move your mouse to the reference variable.
16:40
Or then you can set the controller to be a first order controller.
16:45
I'm assuming you have this in one be uh, linear controller, right?
16:49
With some k change and current integrator. And you could arrive at this, which should be very familiar regulation from 1D.
16:53
Maybe you remember, maybe not. It's basically first order lag linear controller.
17:00
Right. And then if you then just simply consider that the target size right.
17:04
The size of the target, not the distance w right. It's going to basically correspond to the target size of capital W if it's low.
17:11
So then we can basically say do some rearrangement. This is a very simple calculation.
17:19
And what's interesting here is we've messed around with the note here. Rivals to the B log to base to nothing.
17:24
So we basically arrive at something that looks very much like it looks a different.
17:29
And you see that quite elegant, right? So you can actually derive this law from basically a first order linear controller.
17:34
First of all, that is pretty much the same thing. The difference here is that we have this term here, right?
17:41
The gate. Right. So we actually have some more information about why Fitts law was due.
17:47
But we have a richer model right, than an information theoretic model.
17:51
And then you can take this further and you can have a second order controller.
17:56
And that allows you to explain even more features or movement trajectories.
18:00
Right. The particular things people are doing. And you can actually even have an optimal controller.
18:03
And we can learn that from data. And then you can actually very, very accurately not just predict average movement time.
18:08
You can predict the actual movement trajectory. Right.
18:14
The mechanism of movement. So different theories have different levels of explanation.
18:17
Now of a completely different set of theories which are more qualitative.
18:24
It's called dialogue and basically the theories about dialogue.
18:28
It's like human computer interaction is a dialogue. We are telling the system something, pushing factors or speaking to it,
18:31
and in return it's doing something for us, talking to us, or generating information.
18:38
The most basic way of modelling dialogue is through a state diagram, which we stick.
18:43
Diagram of something here for this very old phone. Illustrated in the photograph.
18:48
And you can then reason about it through state diagram formalism like in one way digital circuits.
18:53
What is more useful and actually very useful in the age of AI is actually thinking about dialogue of what is goal directed action,
18:58
which is something the psychologist Donald Newman proposed in the early 1980s.
19:06
But it's very useful even today. The basic theory is that I am in some starting state, and I want to go into some code state to get to the goal state.
19:11
I have to manipulate the system to move that state space towards that goal state.
19:21
Now I have two problems. The first problem is how do I know what is the current state in the system?
19:26
Right? And that is what on a on Omegle.
19:34
Because of evaluation I have to perceive the system, interpret it and figure out what the proof is to current state.
19:36
Now that I know the state right where I am in the state space, I have to figure out how on earth do I get towards the goal state.
19:43
And that is what Donnarumma calls the goal of execution. I have to form an intention, something I want to do, and then I have to specify an action.
19:50
I have a shoe, some action that I want to make, and then I have to actually execute the action correctly.
19:59
And only at that point is the system moving into state space.
20:05
Now I have to overcome the evaluation. I cannot figure out if I go closer to the goal state or not.
20:09
And if you think about it, large language model interactions exactly like this.
20:15
Like your writing unit problem, which is basically overcoming the goals of execution.
20:19
How do I get the system to do what I wanted to do? It does something.
20:23
Who knows what, maybe something you want, maybe not make something you want,
20:27
and then you have to interpret that you have to overcome to go for that location, or you're getting closer to what you actually want.
20:31
Policies. So it's a very rich theory for basically framing computer interaction, human computer interaction as a dialogue.
20:36
Very, very, very, uh, timely with all this chat bots, etc.
20:43
But then you can take it a bit further. So. Your corrective action is just one piece of the puzzle.
20:50
Another thing with AI systems is that AI systems tend to have automated services, right?
20:56
And automated services are difficult to control. When should we initiate an automated service?
21:03
When should be terminated? When should we pause it? How do we change the parameters of the service?
21:08
For this reason, people come up with this theory called mixed initiative interaction.
21:13
The mixed initiative interaction basically sets up both the user and the system can take initiative, the automation.
21:18
And typically it uses graphical user interface principles to allow people to control parameters of the system.
21:26
So a simple thing is in outlook email. If you can email a meeting invitation, outlook can automatically suggest a meeting time for both of you, right?
21:33
That fits both of your calendars and you can be accepted or rejected and so on.
21:41
But a very simple example of a mixed initiative system. Both.
21:45
You can create an invitation order system.
21:49
Both can take initiative. And the question is how do you decide which mixed initiative systems in a good way.
21:53
And to do this, it is obvious that a mixed initiative system has to infer the user's goal accurately.
21:59
And we also have to provide this automation only when very high utility.
22:04
Right? Because only when does high utility of automation humans want to use.
22:10
The other thing we have to think about is that this fourth dimension is triggering at an opportune time, at a time when we really want automation.
22:14
And these are some fundamental things that mixed initiative interaction design is considering.
22:22
And there is a whole host of principles to guide us here.
22:26
I'm not going to go through this long list here now, but I will tell you the gist of what is important.
22:30
One of them is the value add. There has to be high utilities and utilities are overarching principle.
22:36
Another thing we have to basically do is we have to ensure that our mixed initiative interaction is opportune.
22:42
We suggest the right thing at the right place. Another thing we want to do is to allow the user control.
22:47
The user can start, pause, terminate automation and change parameters of the automation.
22:54
We also want to consider uncertainty, the fact that we have to infer the user's going, and we would never correctly infer the use of skill.
23:02
This is a fundamental problem with all of these systems.
23:08
So if you use zoom, for instance, you may notice that if you do something that vaguely resembles a thumbs up,
23:11
you would have to market to generate a thumbs up. That's a mixed initiative system.
23:16
Okay. But it's triggers in inopportune times because we don't have the reliability of it.
23:20
So we also have the idea of having systems learning, right,
23:26
adapting from use and learn the user's preferences and hence be able to to accurately predict when is for high utility for automation,
23:30
when it's an opportune time to do so. Um, have you have automation?
23:38
Uh. Thank you Dave. You know what? Then a completely different aspect of fear of interaction is to use.
23:43
So we view basically user interfaces as tools.
23:54
What we mean by tools is that tools extend our capabilities.
23:59
We are an extension of us. Right? And basically, for a tool to be good for our four things, we have to consider.
24:02
First, a tool has to provide us with utility. It has to give us value.
24:09
Right. Secondly, it's not enough to offer utility.
24:13
If people cannot reach that utility, it's too difficult to use at all.
24:18
They would never realise the utility. So we need to have usability, right?
24:22
Right to reach the utility. So usability is a second aspect.
24:26
Then another problem is that even if we have utility and usability,
24:31
people have to accept the system and basically learn to use this tool to achieve their goals.
24:36
In other words, people have to accept the system. We have to consider acceptability and this is based on practical concerns and social norms.
24:42
Finally, for the tool to actually be accessible to most people, we have to consider accessibility.
24:51
So everybody is disabled. Basically it's something that you have to scale, right.
24:58
And if you make the tools too difficult to use, then nobody would be able to use the tool.
25:02
If you make the tool very accessible, then more people would be able to use the tool.
25:07
So there is actually in this department that we have created an exclusion calculator that can be used to basically find,
25:11
integrate across the population and find automatically how many people can use a tool versus not use a tool.
25:17
It's used by Unilever and these people just so always. The distinction between.
25:23
So usability is about keeping things effective, efficient and safe in general.
25:30
Accessibility is ensuring that our system is usable for a for a majority of people for the maximum amount of people.
25:35
So utility is simple, as I said. Basically you want high utility.
25:47
The match is good. Low utility. Some tall stuff we want to do well not supported.
25:51
And basically when we talk about utility, it's really a relation between the functionality we provide and the user's actually needs and wants.
25:56
And if the match is not good the utility goes down. So utility is not something you cannot just add utility by building a bunch of features.
26:03
If nobody wants these features, the utility is still not there.
26:10
And what people talk about here is getting the right designs versus getting the design right.
26:14
Right. So whether it's functionality in a system and in principle can do what is needed, right.
26:19
That's getting to right. Decide whether people can do anything you practice right is about usability and get into design.
26:24
Right. So even with utility we have to ensure people can reach that utility.
26:30
And that leads to usability. So usability is very different from utility okay.
26:35
And usability is something people have considered a lot in software engineering.
26:40
There is a very established ISO standard. And as we would see later, there are specific processes called usability engineering and out of processes.
26:44
But users with ISO standard, most firms will adhere to this standard.
26:52
So. What is usability?
26:59
Well, first of all, we have an ISO standard. That's the one people tend to use.
27:02
Efficient, uh, effective and safe. Satisfying or safe.
27:06
And usability is most important in relation which are emergent property.
27:11
Just an interaction is an emergent property. Property. Right.
27:16
So usability arises when people are actually using tools to solve problems, uh, for particular tasks.
27:19
So usability is relational, but it's also emergent, right?
27:27
It only happens when people are actually trying to use the tool.
27:32
So it's very difficult to just say something is usable. You have to study the emergent usability in in situ.
27:36
Usability is also measurable. We can measure usability and it's multidimensional.
27:44
Or you can see some of the sample dimensions here with two different views of usability.
27:50
So here is a very simple example. Often in practice this is not really a research tool because it's not a very well validated survey.
27:57
But often people want to get a sense right a quick and dirty as they say.
28:05
Usability scale is to be system usable not and then people use can use this system usability
28:09
scale issues ask a bunch of questions outputs a number and this based on this number.
28:14
There's a fresh tone on what is considered acceptable usability or not.
28:19
Is it perfect? Note does it give you some cheap and cheerful indication early on in the design process?
28:23
Yes. But even if as utility and usability, we have to think about acceptability, right?
28:30
So people should just use particular tools. You may not choose to use, for instance, certain mobile phone features.
28:40
Right? Or you may turn off autocorrect if you don't like it, and so on.
28:45
Right. So there is a concern about the acceptability of the tool, whether people choose to use the tool given a choice.
28:48
And then there are two different ways of viewing acceptability.
28:57
First, practical acceptability is the tool actually fit for purpose is to cost right.
29:00
Is it compatible with other systems. And so one is it does it take a long time for me to learn to use the tool?
29:06
And then there is a more complicated way of thinking about it's called social acceptability.
29:12
This webinar interactions actually map well to social norms how we are supposed to do things.
29:17
So one example of social acceptability is extended reality glasses.
29:23
They usually have gestures. And if you're standing there I don't know.
29:27
I think we did a study on the glass for you. And you basically use gestures like we among other people.
29:31
You look like a crazy person. So it has a very low social acceptability,
29:37
but it has a pretty high practical acceptability when assessed in lab situations where nobody can see you're moving about.
29:41
So act of acceptability includes this choice not to use a system.
29:48
And non-use is a big issue in practice.
29:52
It makes a lot of people lose a lot of money because people simply refuse to accept systems.
29:55
This happens a lot and we will see this later when we talk about appropriation.
30:02
So this this comes down to technology adoption. Accessibility.
30:07
Finally, we all. It means basically that we want to make the systems accessible to as many people as possible.
30:14
So everybody is disabled and disability is very multidimensional.
30:20
And we're also situation here because for instance, if I'm holding two bags I can't use my hands.
30:24
Right. So disability is just something that is like everywhere in society.
30:29
And it's just a question of where people are in this kind of multidimensional disability space.
30:33
So, uh, increasingly people have realised,
30:39
but if they make systems that are not accessible for certain individuals or certain characteristics of certain pretzels,
30:42
they lose a lot of money, because if you tried to sell a door and people can't open the door, says Unilever, then you don't sell the job.
30:48
If you if your website is confusing as tiny text, people don't buy things.
30:55
So people have actually been tracking these big corporations, which is about making money and making things more accessible tends to make firms money.
31:00
So there is a huge monetary incentive, not to mention ethics, of course, to actually make things accessible.
31:08
And there's also increased regulation in most of the world.
31:14
A challenge in accessibility is that people are individuals and people have individual disability.
31:17
So of course, individual combinations of disabilities.
31:22
And this can make it very tricky to design accessible systems or even evaluate them appropriately.
31:25
So there's a lot of ongoing research about this.
31:30
We talked a little bit about AI systems and we talked about mixed initiative, interaction and dialogue.
31:36
But it turns out we also know a lot about interaction in terms of interacting with AI systems through theories of automation.
31:41
Now, these fears of automation have developed since the 1940s, actually,
31:49
and in the 1960s really took hold with the development of cybernetics, which later became more than control theory.
31:54
So there's a lot we can learn from the theories of automation.
31:59
So what is automation? So automation is the allocation of tasks to machines.
32:03
Okay. So there are lots of everyday.
32:08
But automation systems you probably use GE made or some similar email and you may notice that there is a spam folder.
32:12
So that's just to sure. Like the system is scanning your emails and determining which image you do not want to see, and puts them in a spam folder.
32:18
If you do autocomplete, I could type as URL in a browser. That's a form of automation.
32:26
If you're typing on a keyboard and that's autocorrect, that's automation.
32:31
And of course we can have higher automation as well. We can have all kinds of driver assist systems etc.
32:35
Right. So automation is pretty much everywhere.
32:41
Right. But the trick with automation is what kind of automation is appropriate, right?
32:45
What type of automation is appropriate because there are multiple types and how much automation is suitable?
32:50
And how do we know?
32:56
What should we look for when we determine the type and level of automation but is suitable for the functions, you know, system design.
32:57
And this is something where we can tackle using the types and level of automation theory.
33:05
So basically this is a framework.
33:10
We determine the type of automation I do acquisition automation analysis decision action or a combination of local adaptive
33:13
automation and these types of automation that's going to want to call what's called the human information processing.
33:20
So acquisition automation is about sensing. So that's maps on to sensory processing of a human analysis.
33:26
Automation is about making sense about what you see forming an integrated construct.
33:33
So analysis automation maps on to perception or working memory.
33:39
Analysis. Automation typically means inference. For instance, decoding something writing in a bush and I can see a tiger in the bush.
33:44
That's the analysis I'm doing. Inference basically object recognition in my brain.
33:53
To see there is a tiger in the bush. Okay.
33:57
But the next stage decision of the which maps to decision making, is about making a choice to assemble and potentially be eaten by a tiger.
34:00
Do I run away? Right. So these decisions have costs, right?
34:08
So the difference between analysis automation and decision automation is cost analysis.
34:12
Automation is making inference. Decision.
34:18
Automation is making a decision, and every decision has to have a trade or some benefits and of course otherwise to the decision.
34:22
So that's the difference between analysis and decision automation.
34:30
Finally, having made a decision, I have to execute. And that's actually like running away from the title.
34:33
Now if you think about a system and you abstract the level of complexity.
34:41
And you forget about all the specific solutions that you have in your system, and you just think about the functions, what your system has to do.
34:47
You can go look for all of these functions.
34:57
You cannot determine what is the Shubert information, some information processing stage of all of these functions.
35:00
At that point, you can use this table to map it to a type of automation.
35:07
No. Now, you know the types of automation that could be applicable in your system,
35:12
but you don't know the degree of automation that is appropriate to do that.
35:16
Look at the other table here. The levels of automation ranging from no assistance to human, does everything to a fully autonomous system.
35:20
What a system does everybody. So they go through all the functions in our system.
35:29
Think about what is the type of automation, what is applicable.
35:34
And then the reason is at the same time about what is the level of automation, what is desirable in business.
35:38
This gives us an initial type of level of automation okay.
35:44
Now at this point. We can, then this is.
35:48
Just say what I just said. No, we can do. No, we can sort of critique of system.
35:52
And the problem here is knowing what to look for. What should I think of when I could take my system.
35:57
And these frameworks gives you the answers. So it says you should first concede on user centric criteria and without for user centric criteria.
36:01
The first one is mental workload. So how much mental workload do you demand from the user?
36:10
This is a very straining cost for a very easy task given the automation type and level of the site.
36:16
The second part is situation awareness. So usually automation is used to suggest decisions of doing task for you.
36:23
Right. And situational awareness is about knowing something about the task structure.
36:31
So for example, if I have an automated system that says I shouldn't do a particular thing if I don't see the underpinning data based on my position,
36:36
I have very low situational awareness.
36:45
Maybe that data is based on nonsense, maybe a table where all the cells are filled up at zero because there was no data available.
36:47
What? A sensor that fed the data was broken, right?
36:53
So situational awareness, just how much situational awareness can this system actually provide to the user?
36:56
So the user can be aware of everything that is relevant for the task.
37:01
So that's the second criteria criterion.
37:05
The third one is very important and it's called complacency.
37:09
So basically complacency is an unconscious typical process where we gradually become more and more complacent.
37:13
So whenever we think so our agent can do our job.
37:20
We're all alert. We tend to feel complacency. And a strength agency measures how much we think we matter to the world after the task.
37:23
If you reduce agency, people's sense of ownership of actions and how they matter.
37:33
People tend to be complacent and it tends to be an unconscious effect.
37:38
This is why when you go into Stansted airport and you go through security often,
37:42
you know the metal detectors are triggering even though there's nothing there, right?
37:46
That's all done by design because they were randomly triggered all the time to keep people on their toes, to reduce complacency.
37:50
So complacency has to be managed. Air traffic control has learned this the hard way.
37:57
By the way. There's lots of mechanism involved just to reduce complacency.
38:01
So we have to think about the risk of complacency which again typically is unconscious.
38:05
You can't tell people don't be complacent. That doesn't work right.
38:10
We are trying not to be complacent, but they would be unconsciously complacent.
38:13
The fourth hazard is what's called skill degradation. Basically, if we automate stuff and you never do it, you forget property.
38:17
You lose the ability to do things. If you never read a book, eventually you can't read.
38:26
Basically, you can't read a longer argument if you never are able to pay attention to things for a prolonged amount of time.
38:30
You lose the ability to actually use your attention to pay attention, right?
38:37
We lose abilities we do not use, and we have to think about the long term consequences of that.
38:42
Now, having then applied this user centric criteria and discussed in a team,
38:49
possibly done some experimentation of consulting psychologist or domain experts,
38:54
we then realise OG or initial automation design is a bit risky, right?
38:58
That seems to be a lot of concerns about mental workload or whatever, right?
39:03
So we're going to go back and we may change the type and level of automation into functions in our system,
39:07
or even add additional functions to prevent negative user centric things from happening.
39:13
At that point, we can also apply the system level criteria either only to.
39:19
The first one is about the reliability of automation. So how trustworthy is this automation?
39:24
How robust is it for the variety of situations that are relevant for deployment?
39:29
And the second one is about risk. What are the cost of these different outcomes.
39:33
Right. What are the cost of decisions and outcomes given to potential automation?
39:38
And of course, if the balance of risk is not there, we probably should have automate a partner.
39:43
And based on this we can again up to two types of level of automation.
39:47
And we do this in an iterative way. And at the end we have a very nice automation design for our system.
39:51
So this framework is called the types and level of automation. And here you can see a step step to use the marketplace.
39:57
And here is a horrible diagram from a regional paper telling you how to do it.
40:05
Which is exactly what I just said. It's a very, very good, cheap and cheerful way of building a system.
40:08
In practice, I can tell you what the real challenge is.
40:14
If you go into a company and they say we want to use AI or whatever, right, to make things more productive,
40:17
the first problem is that they have no idea what people are actually doing. Most companies do not know what they are actually doing, right?
40:23
They don't have proper processes, right? So they don't really know what they are doing.
40:29
And if you don't know what you're doing, you don't know which functions. People are carrying it, carrying things out.
40:33
And the way these functions are dependent on each other, if you don't know the functions, what you have to do at certain points,
40:38
you don't really have any functions in a serious way to basically apply the types of automation
40:45
framework to the number one thing when applying automation is you need to understand a system,
40:51
which is something we're going to talk about in lecture seven.
40:57
When we talk about system mapping, without an understanding of the system, we cannot apply an automation framework.
40:59
But if you have an understanding of the system we have mapped out, you know, all the key functions.
41:05
How we depend on each other, how they relate. This automation framework is a very good way to reason about automation in a very sophisticated way.
41:10
Okeydoke. So that's one other theory of interaction, but it can be applicable.
41:19
And then one of the most underestimated things that the most important things in practice is practice.
41:24
How do we actually interact with systems in practice?
41:31
Because if we don't understand how people in practice interact with systems,
41:34
we're going to provide systems that people are not going to adopt or appropriate, and things that people don't appropriate to adopt or reject.
41:38
And this happens a lot.
41:45
So there's estimations that about half of commercial available systems kind of fail because people do not appropriate and adopt the systems.
41:47
And that's because we have a lack of understanding of practice.
41:54
So interactive systems number one are very flexible.
42:00
You can do a lot right. There's lots of ways you can send an e-mail.
42:04
That's right. Lots of different ways. You have to pick a particular way of doing things.
42:09
If you just think about the sheer degrees of freedom, you're available, right?
42:13
For doing any task, it's amazing we actually are able to do anything at all.
42:17
There are so many different ways we can go about achieving tops.
42:21
Right. So how do we do it? Right? How do we choose how to do things in this huge space of options that we have?
42:25
Well, the answer is practice, right? We do things because we know how to practice our art.
42:33
So here is a classic example. This is a very famous paper from early kowski who is a professor at MIT.
42:41
Or is the title of this? Uh, she did one of the pioneering studies in a very long time ago.
42:47
I think that's what we should be 1994, if not 95.
42:54
Um, basically what she did is, uh, I don't know if you heard about Lotus Notes, which are email software, but you're too young to know about it.
42:57
You're your colleagues if you're working in an office. Lotus notes is basically like a precursor to Gmail.
43:05
So in the old days, you had your email system and it was on your personal computer under a hard right, right?
43:12
Lotus notes is an email interface which is actually sitting on top of a database shared with multiple people.
43:18
It was originally by Lotus, but Lotus was acquired by IBM.
43:25
So if you worked at IBM, they have 300,000 employees. You would have to use Lotus Notes.
43:28
And it's a very strange email software because it doesn't work like any other email software,
43:32
because it gives you a view of emails based on a database.
43:37
Right? This means when you send emails, copy people in flat screens and so on,
43:40
all of these things are visible to anybody else who has a view of that particular email, right?
43:45
So it allows people to be collaborative through email use.
43:51
Now in the 1990s, people got very excited about what's called groupware collaborative systems, because the idea is that people have more awareness.
43:54
Remember we talked about theories of collaboration before. Um, cooperation and awareness is a big thing, right?
44:03
People need to be aware of what everybody else is doing. And to do that we need boundary objects, right?
44:09
Something like a blackboard or something. But everybody can look at the unified view.
44:14
Well, Lotus Notes is a boundary object that conveys potential to raise awareness.
44:18
So people try to basically, uh, use with Lotus, not Lotus,
44:23
not suffering big organisations to get people to be more aware of what is going on and increase collaboration.
44:28
And I hear you say that people are more productive and the company makes more money.
44:34
What what you did is that she went to the American consulting firm and think, maybe you can tell them we got to,
44:37
you know, just know if they accepted and we're going to make lots of money and be really efficient.
44:44
What is very interesting is that you found now, this is early days when people weren't really aware of the importance of practice is what
44:50
basically what people do in practice is they do whatever the purpose of their purpose is,
44:57
of course, right. People have goals, and people achieve their goals in whatever way they think it's going to serve of their goals,
45:01
not other people's skills like their own goals, typically. Right.
45:09
So of course, what happens in a consulting firm is that people don't want to share information because consulting is all about billable hours, right?
45:13
Sharing information and dealing with Lotus Notes is work, but doesn't directly benefit them.
45:20
It probably benefits the company as a whole over the long term, but in the short term, for an individual consultant,
45:25
I'm better off not using this tool and just getting on with my thing and getting billable hours, right.
45:31
So basically how individuals think about these systems influence how they adopt the system, right.
45:38
So they would people would use a one key finding here for people to use their understanding of the system to make decisions.
45:44
Does this system further my own goals or not? And if their understanding is flawed, for instance, what could be correct but often is flawed?
45:52
Actually, this system will not further my own goals. I'm going to reject this system.
46:00
That's the most common move people make, right? So people need to be able to understand the system.
46:04
I know in the 1990s nobody understood. If you ever use Lotus Notes, it's also very confusing email email system.
46:09
So people did not understand the system. And you don't really you don't really want to adopt the system.
46:15
You do not understand at the level of some form of trust, right?
46:21
So basically people also try to interpret systems based on their prior experience.
46:27
So they use the prior experience of EA monopoly who are both collaborating and consulting, and they try to fit those ideas to this new system.
46:32
But they couldn't do it because they couldn't do it. They didn't adopt it.
46:39
The other thing is that the organisation of the company itself influences how the system ends up being used, right?
46:44
So you have to set up reward policies, etc., right, to make it beneficial to use the system.
46:50
This was not the case. And there was also no leadership signal from the top that this is a good thing and you should use it.
46:56
And this is the training you get to use the system. And this is why we will all benefit.
47:02
And we know it's that sort of advocacy. It's going to be very hard for people to adopt systems.
47:06
And this is what we see in organisations all the time.
47:11
You probably are too young to be experienced this in the corporate world or here at the university,
47:15
but if you procurement system is a classic thing, but people hate the title, right?
47:19
So if you procure meaning you buy equipment, you need to get free quotes, you need to put things in a system.
47:24
You need to use very specific suppliers for some reason, and you need to think about a lot of different things.
47:30
And there are very, very, very terrible interfaces for doing so.
47:35
So what happens in practice? People use a corporate credit card to buy the equipment completely out of process.
47:39
What if I do something on pen and paper and pick up the phone right and do another one?
47:45
People figure out ways to achieve a goal. What's wrong with the system if the system doesn't really further and further that serve their own goals?
47:49
And this is a very important finding in practice, right?
47:57
People need to understand assistance with some degree of understanding what is sufficient for them to be confident they can adopt this.
48:00
Now they need to be considerations of the organisational processes and the values
48:07
of the work policies of the organisation for people to up to adopt systems.
48:13
So we basically have this idea of adoption and adoption, right?
48:20
So what people do is people fiddle with systems in order to make the system works of their purposes.
48:25
So we use, for instance, digital cameras for lots of different things.
48:30
We use an email inbox as a to do list by marking unread emails as unread red emails unread.
48:34
So we have an informal to do list. And so we basically improvise to figure out how to do our work.
48:40
And there are different ways we can do this. We can do personalisation. So personalisation means non-functional changes.
48:47
For instance changing your desktop background. That's an example of personalisation.
48:53
And also ways personalisation can affect face.
48:58
We can have cognitive effects because we can maybe improve the systems that we think is easier to use based on how we organise it for all purposes.
49:01
There are social effects.
49:08
For instance, we can have like some sort of thing that we have in common with other people and we can highlight vascular systems like,
49:09
I don't know, a football team or something like that. And that can be emotional.
49:16
Things like the feeling of like hedonic properties,
49:20
like the feeling of something being fun or engaging or positive, reducing boredom, etc. that's personalisation.
49:23
Another thing we can do is functional modification of that is known as tailoring.
49:31
So this basically means that we basically tinker with the system to make the system serve all purposes.
49:35
There are three particular ways people tend to do is we do customisation.
49:41
So we basically can change the attributes to serve a functional purposes.
49:46
We do integration. So we add new functionality to systems to be able to be able to, uh, to be able to do new things.
49:50
For instance, we can create macros and keep key keyboard accelerators or write little scripts and so on.
49:58
And then finally we can do extensions. So we can add completely new powers to our system to fit our purposes.
50:05
And then the most important thing of all is appropriation.
50:16
So appropriation is a very simple idea. It's basically describes how adaptation and adoption processes occur.
50:20
All right. And they may change workflows. Division of labour collaboration coordination organisational processes and norms.
50:28
Right. So our appropriation does not really concern modifying interactive systems.
50:36
It concerns all the associated changes.
50:42
Right. What happens when you change interaction in practice?
50:46
And appropriation is incredibly likely to happen in practice.
50:51
So appropriation basically means.
50:55
If I'm giving on you to honour your system, I'm going to figure out whether that technology system is fit for my purposes.
50:58
And if it is, how should I use that technology system to achieve my goals?
51:04
Okay. In my organisation. Now, if you think about it, appropriation is really very likely to happen in practice for two reasons.
51:08
First of all, why do people appropriate to begin with.
51:17
Because the system or the technology we provided, it is not 100% fit for purpose, right?
51:23
It does something, but it doesn't do it quite right for me. Right.
51:28
Why does that happen? Well, first of all, the design team doesn't have a perfect crystal ball.
51:32
They cannot imagine every use context every particular user needs and wants, every organisational constraints and so on.
51:37
Right. So that would always be a mismatch in what the design team builds and what people would actually have to use.
51:43
Right. And if there's a mismatch, people have to engage in appropriation to figure out how to do that.
51:49
Work with your system. Right. Second, even if the design team has 100% perfect crystal ball, they will never be able to 100% predict.
51:55
How things will change, right? In practice. Processes change.
52:07
Needs and wants change. We live in a dynamic world. So appropriation is in practice, always necessary.
52:11
And that means we've figured out how to use systems often counter to its intended purpose to achieve our goals.
52:17
An important thing is if we can't figure out how to appropriate a system, we will react to systems because they cannot be used to achieve our goals.
52:25
So there are lots of appropriation myths. People can be a champion for a system.
52:35
So they say this is a great system. You should use it for reasons x, Y and z.
52:40
People may also substitute parts of a system with another system, because they don't think that a system works very well.
52:44
They can also use a completely different way of accomplishing to talk, to basically reject a system which is very common.
52:50
They can criticise the system by comparing it to other ways of doing the work.
52:56
And they can interpret the systems. So they can, for instance, explain functionality.
53:00
The classic example is password rules. So this lecture computer has a different possible rule than the rating for some stupid reason right.
53:04
They haven't explained why that is, which annoys me. If you explain I've always a good reason, which I suspect there is not.
53:13
I would be more accepting of this kind of thing, right? So we want to try to interpret systems, right?
53:18
We can also people can also can attempt to make others reject the system or prevent the usage or be deliberately slow in taking up the system.
53:24
Fortunately, people have thought about this and overall guidelines for appropriation, but are very generic but quite useful.
53:33
So one of the things is allow interpretation, allow people to interpret the system and then basically assign meaning to the system.
53:39
And you can do this in lots of different ways.
53:47
Most of may times allow you to colour image for instance, and the colour of your choice what the chorus actually mean.
53:49
Another thing is to provide visibility.
53:55
It's very difficult to appropriate a system that is a complete black box, because we have no idea what is going on.
53:57
So it's good to explain mechanisms. So it's also very good and related to back to expose intentions.
54:02
So instead of driving users crazy by insisting on insane password rules, explain what these password rules unnecessarily write.
54:09
That will increase acceptability. Another important thing, which is really a lesson in life if nothing else, is support, not control.
54:17
A very good mantra. It's very difficult to control things.
54:25
The problem if you control things. Yes, but you better be right.
54:29
Right. So one way is that you have a very intricate user interface process.
54:33
You have to go through everything in a particular way. That is a very, very dangerous way to build a system.
54:37
Because if that process, which is highly prescriptive, is not fit for purpose, people have no choice but to the system.
54:43
So you want flexibility. So you want to support people in doing things in different ways, not insisting that people do it in a specific way.
54:51
It's a little bit like you want flexibility. A system without flex has to be 100% right all the time, but it's very difficult to shift.
54:59
So you want flex in the system. You can also allow people to, you know, put the use plug in some reconfigure the system encouraging sharing.
55:07
So if people figure out how to appropriate a system, they can share these operations with other people and learning from a corporation.
55:15
So when we deploy the systems, we can learn how people use the system and gain design,
55:22
know how to build better systems that are more likely to be I don't. And that was the last, uh, theory of interaction I'm going through here.
55:25
I should say there are other theories of interaction, but I've left out of this course because we only have so many, so much time.
55:33
For instance, there are theories about how people search for information quickly,
55:39
information for the number of theories about how people reason about things called rationality, which is highly mathematical but quite interesting.
55:42
And if you're interested in about grasshoppers in the book, about that particular part of the course, thank you very much.
55:50
And the next time we're going to talk about user interfaces. So how to build user interfaces for a large variety of devices and tasks.
55:55
Okay. Thank you very much. And I'll see you next time. I had.